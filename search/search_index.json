{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Lund University Modular Inversion Algorithm (LUMIA) is a python package for performing atmospheric transport inversions. The release 2020.8 is described in https://www.geosci-model-dev-discuss.net/gmd-2019-227/ can be downloaded here , however, we recommend instead getting the latest commit from github : git clone --branch master https://github.com/lumia-dev/lumia.git This documentation focuses on the current version of the code. Folder structure The folder structure of LUMIA is the following: the lumia folder contains the lumia python module (i.e. what you get when doing import lumia in python) the transport folder contains the transport python module (that you can import using import transport in python), which contains the pseudo-transport model used in our LUMIA simulations (see documentation) the docs folder contains a more extensive documentation the run folder contains example scripts and configuration files. the gridtools.py file is a standalone module (accessed via import gridtools ) The other files and folders are either related to optional functionalities ( icosPortalAccess ; src ), required for the functionality of the LUMIA web page ( CNAME , mkdocs.yml ), or for the structure of the python package ( setup.py , LICENSE ). The Makefile is more for information purpose than for being used ... lumia, LUMIA, transport and transport The LUMIA git repository contains two python packages ( lumia and transport ). Furthermore, the lumia package contains several modules called transport . This can be very confusing. Therefore, throughout the documentation: the transport package refers to the top-level tranport package (i.e. what is imported via import transport ) the transport module refers to the lumia.models.footprints.transport module. \"the lumia package\" refers to lumia (what is accessed via import lumia ) LUMIA refers to the entire project. Installation LUMIA is written in python, and depends on many other scientific packages. We recommend using in a miniconda virtual environment, with at least the cartopy package installed: # Create a conda environment for your LUMIA project (change `my_proj` by the name you want to give it) conda create -n my_proj cartopy # Activate the environment: conda activate my_proj # Clone lumia from its git repository (change `my_folder` by the name of the folder you want LUMIA to be installed in. The folder should not exist before) git clone --branch master https://github.com/lumia-dev/lumia.git my_folder # Install the LUMIA python library inside your virtual environment (replace `my_folder` by the name of the folder where you have cloned LUMIA in). pip install -e my_folder Dos and Don'ts Do get familiar with how python packages should be installed https://docs.python.org/dev/installing/index.html Do get familiar with how conda environments work: conda documentation conda cheat sheet Don't try to use the Makefile directly (but you can look at it, it contains a shortened version of this documentation). Testing the installation Once you have installed lumia in your python environment (i.e. once you have gone through the installation instructions above ), the lumia module will be accessible on your system. Try for example: conda activate my_proj # Make sure you are in the right python environment! cd /tmp # Move to another folder, basically anywhere where your lumia files are not python -m lumia # Run python with the `lumia` module This should produce an error such as: /home/myself/miniconda3/envs/my_proj/bin/python: No module named lumia.__main__; 'lumia' is a package and cannot be directly executed This is good! it means that python has found lumia (but doesn't know what to do with it, that's another issue). If you get instead an error such as: /usr/bin/python3: No module named lumia This means that python cannot find the lumia module: either you are not within the right python environment (read the conda documentation if you are not familiar with it), or that another error happened during the installation of lumia (did you use the pip install -e /path/to/lumia as instructed above? did that return an error (maybe some dependency could not be installed?)). Recommended workflow The run folder contains example scripts and configuration files. You can use them as an example on how to start (in addition of reading the documentation that exists). You can also put your own scripts and data in that folder. We can recommend a few alternative workflows, depending on what you plan to do with LUMIA: You can put your scripts and data directly under the run directory (e.g. /home/myself/lumia/run ). This is a good way if you are just starting with LUMIA, of if you plan to develop it further. You can install lumia in one folder (e.g. /home/myself/libraries/lumia ), and put your scripts in a completely different folder (e.g. /home/myself/projects/my_fancy_project ): if you have installed it correctly, the lumia python package is available from anywhere on your system (provided that you have activated the python environment in which it is installed). This is a good way if you plan to work on LUMIA from several different projects. If you plan to actively develop LUMIA, you could also have separate installation (e.g. /home/myself/lumia_stable and /home/myself/lumia_dev ), each installed in a different python environement (e.g. my_old_env and my_new_env ). Documentation summary LUMIA implements the inversion as a combination of (semi) independent modules, implementing the various components of an inversion setup. The links below point to the documentation of these modules: Theoretical summary LUMIA is (primarily) a library for performing atmospheric inversions: Observations of the atmospheric composition are used to improve a prior estimate of the emissions (or fluxes) of one (or several) atmospheric tracer (e.g CO \\(_2\\) ). The inversion relies on an atmospheric chemistry-transport model (CTM) to link fluxes of a tracer in and out of the atmosphere with observed atmospheric concentration. This can be formalized as \\[y + \\varepsilon_{y} = H(x) + \\varepsilon_{H}\\] where \\(y\\) is an ensemble of observations, \\(H\\) is a CTM, and \\(x\\) is a vector containing parameters controlling the CTM (i.e. x is the control vector ). The uncertainty terms \\(\\varepsilon_{y}\\) and \\(\\varepsilon_{H}\\) represent respectively the measurement error and the model error. The aim of the inversion is to determine the control vector \\(x\\) that leads to the optimal fit of the model \\(H\\) to the observations \\(y\\) . This is formalized as finding the vector \\(x\\) that minimises the cost function \\[J(x) = \\frac{1}{2}\\left(x-x_b\\right)^TB^{-1}\\left(x-x_b\\right) + \\frac{1}{2}\\left(H(x) - y\\right)^TR^{-1}\\left(H(x) - y\\right)\\] where \\(x_b\\) a prior estimate of the control vector \\(x\\) (that we are trying to determine). The error covariance matrices \\(B\\) and \\(R\\) contain respectively the estimated uncertainties on \\(x_b\\) and \\(y - H(x)\\) . The optimal control vector \\(\\hat{x}\\) is determined using an iterative conjugate gradient approach. While the aim of the inversion is typically to estimate the emissions of a tracer, the control vector \\(x\\) rarely consists (directly) of the emissions: it can for instance contain scaling factor or offsets, at a lower resolution than the emission themselves, it may contain only a subset of the emissions, it can also include additional terms such as an estimate of the boundary condition, bias correction terms, etc. \\[ E = M(x) \\] lumia.observations implements the observation vector ( \\(y\\) ), the corresponding uncertainties ( \\(R\\) ); lumia.models handles the interface between LUMIA and the actual CTM ( \\(H\\) ); lumia.prior implements the prior control vector \\(x_b\\) and its uncertainty matrix \\(B\\) ; lumia.optimizer implements the optimization algorithm; lumia.data implements the emissions ( \\(E\\) ) themselves; lumia.mapping implements the mapping operator ( \\(M\\) ), that is used to convert between model data and control vector. lumia.utils contains various utilities, used by several of the other lumia modules. The Utilities page also describes the gridtools module, that is distributed along lumia . transport acts as a CTM ( \\(H\\) ) in our LUMIA inversions. Most of these modules can be used independently. For instance, the lumia.observations defines an Observation class that can be used to read, write and analyze observation files used in LUMIA (including LUMIA results). Example scripts showing a full inversion are provided under the run directory, and presented in more details in the Tutorial . Finally, some top-level objects (classes and functions) are described under the LUMIA overview page. The page also contains a more elaborate information on the coding strategy of LUMIA. LUMIA Development strategy LUMIA was designed as a modular system: it should remain as easy as possible to replace the default CTM and the individual components (prior, observations, optimizer, etc.) should remain as independent from each other as reasonable. In practice this means that the following technical choices were made: The LUMIA modules contain python classes to handle the different components of the inversions (e.g. an Observations class to handle the observations, a PriorConstraints class to construct and store the prior uncertainty matrix, etc.). All modules implement or support alternative versions of these classes. For instance, the \"Observations\" class is defined in the lumia.observations.observations class (i.e. lumia/observations/observations.py file): This approach makes space for alternative implementations (e.g. one could implement an Observations class part of a lumia.observations.satellite_obs module). It can however make imports paths length ( from lumia.observations.observations import Observations , from lumia.models.footprints.data import Data , etc.). To circumvent this, many \"shortcuts\" have been defined in the __init__.py files of the module (enabling e.g. from lumia.observations import Observations ), and, even in the top-level __init__.py file ( lumia/__init__.py ). These are documented in the relevant sections (e.g. in Observations ) and, for the top-level objects, in the section below. In several of the modules, there is a protocol.py file, which essentially contain templates for the various classes. In theory, as long as alternative classes follow the template defined in the protocol.py files, they should not lead to major compatibility issues. Although it was developed along LUMIA, the code that acts as a CTM ( transport ) remains a separate python package and is run as a subprocess: This forces us to limit the degree of inter-dependency between the two codes, and thus should make it easier implementing an alternative CTM.","title":"Home"},{"location":"#folder-structure","text":"The folder structure of LUMIA is the following: the lumia folder contains the lumia python module (i.e. what you get when doing import lumia in python) the transport folder contains the transport python module (that you can import using import transport in python), which contains the pseudo-transport model used in our LUMIA simulations (see documentation) the docs folder contains a more extensive documentation the run folder contains example scripts and configuration files. the gridtools.py file is a standalone module (accessed via import gridtools ) The other files and folders are either related to optional functionalities ( icosPortalAccess ; src ), required for the functionality of the LUMIA web page ( CNAME , mkdocs.yml ), or for the structure of the python package ( setup.py , LICENSE ). The Makefile is more for information purpose than for being used ... lumia, LUMIA, transport and transport The LUMIA git repository contains two python packages ( lumia and transport ). Furthermore, the lumia package contains several modules called transport . This can be very confusing. Therefore, throughout the documentation: the transport package refers to the top-level tranport package (i.e. what is imported via import transport ) the transport module refers to the lumia.models.footprints.transport module. \"the lumia package\" refers to lumia (what is accessed via import lumia ) LUMIA refers to the entire project.","title":"Folder structure"},{"location":"#installation","text":"LUMIA is written in python, and depends on many other scientific packages. We recommend using in a miniconda virtual environment, with at least the cartopy package installed: # Create a conda environment for your LUMIA project (change `my_proj` by the name you want to give it) conda create -n my_proj cartopy # Activate the environment: conda activate my_proj # Clone lumia from its git repository (change `my_folder` by the name of the folder you want LUMIA to be installed in. The folder should not exist before) git clone --branch master https://github.com/lumia-dev/lumia.git my_folder # Install the LUMIA python library inside your virtual environment (replace `my_folder` by the name of the folder where you have cloned LUMIA in). pip install -e my_folder Dos and Don'ts Do get familiar with how python packages should be installed https://docs.python.org/dev/installing/index.html Do get familiar with how conda environments work: conda documentation conda cheat sheet Don't try to use the Makefile directly (but you can look at it, it contains a shortened version of this documentation).","title":"Installation"},{"location":"#testing-the-installation","text":"Once you have installed lumia in your python environment (i.e. once you have gone through the installation instructions above ), the lumia module will be accessible on your system. Try for example: conda activate my_proj # Make sure you are in the right python environment! cd /tmp # Move to another folder, basically anywhere where your lumia files are not python -m lumia # Run python with the `lumia` module This should produce an error such as: /home/myself/miniconda3/envs/my_proj/bin/python: No module named lumia.__main__; 'lumia' is a package and cannot be directly executed This is good! it means that python has found lumia (but doesn't know what to do with it, that's another issue). If you get instead an error such as: /usr/bin/python3: No module named lumia This means that python cannot find the lumia module: either you are not within the right python environment (read the conda documentation if you are not familiar with it), or that another error happened during the installation of lumia (did you use the pip install -e /path/to/lumia as instructed above? did that return an error (maybe some dependency could not be installed?)).","title":"Testing the installation"},{"location":"#recommended-workflow","text":"The run folder contains example scripts and configuration files. You can use them as an example on how to start (in addition of reading the documentation that exists). You can also put your own scripts and data in that folder. We can recommend a few alternative workflows, depending on what you plan to do with LUMIA: You can put your scripts and data directly under the run directory (e.g. /home/myself/lumia/run ). This is a good way if you are just starting with LUMIA, of if you plan to develop it further. You can install lumia in one folder (e.g. /home/myself/libraries/lumia ), and put your scripts in a completely different folder (e.g. /home/myself/projects/my_fancy_project ): if you have installed it correctly, the lumia python package is available from anywhere on your system (provided that you have activated the python environment in which it is installed). This is a good way if you plan to work on LUMIA from several different projects. If you plan to actively develop LUMIA, you could also have separate installation (e.g. /home/myself/lumia_stable and /home/myself/lumia_dev ), each installed in a different python environement (e.g. my_old_env and my_new_env ).","title":"Recommended workflow"},{"location":"#documentation-summary","text":"LUMIA implements the inversion as a combination of (semi) independent modules, implementing the various components of an inversion setup. The links below point to the documentation of these modules: Theoretical summary LUMIA is (primarily) a library for performing atmospheric inversions: Observations of the atmospheric composition are used to improve a prior estimate of the emissions (or fluxes) of one (or several) atmospheric tracer (e.g CO \\(_2\\) ). The inversion relies on an atmospheric chemistry-transport model (CTM) to link fluxes of a tracer in and out of the atmosphere with observed atmospheric concentration. This can be formalized as \\[y + \\varepsilon_{y} = H(x) + \\varepsilon_{H}\\] where \\(y\\) is an ensemble of observations, \\(H\\) is a CTM, and \\(x\\) is a vector containing parameters controlling the CTM (i.e. x is the control vector ). The uncertainty terms \\(\\varepsilon_{y}\\) and \\(\\varepsilon_{H}\\) represent respectively the measurement error and the model error. The aim of the inversion is to determine the control vector \\(x\\) that leads to the optimal fit of the model \\(H\\) to the observations \\(y\\) . This is formalized as finding the vector \\(x\\) that minimises the cost function \\[J(x) = \\frac{1}{2}\\left(x-x_b\\right)^TB^{-1}\\left(x-x_b\\right) + \\frac{1}{2}\\left(H(x) - y\\right)^TR^{-1}\\left(H(x) - y\\right)\\] where \\(x_b\\) a prior estimate of the control vector \\(x\\) (that we are trying to determine). The error covariance matrices \\(B\\) and \\(R\\) contain respectively the estimated uncertainties on \\(x_b\\) and \\(y - H(x)\\) . The optimal control vector \\(\\hat{x}\\) is determined using an iterative conjugate gradient approach. While the aim of the inversion is typically to estimate the emissions of a tracer, the control vector \\(x\\) rarely consists (directly) of the emissions: it can for instance contain scaling factor or offsets, at a lower resolution than the emission themselves, it may contain only a subset of the emissions, it can also include additional terms such as an estimate of the boundary condition, bias correction terms, etc. \\[ E = M(x) \\] lumia.observations implements the observation vector ( \\(y\\) ), the corresponding uncertainties ( \\(R\\) ); lumia.models handles the interface between LUMIA and the actual CTM ( \\(H\\) ); lumia.prior implements the prior control vector \\(x_b\\) and its uncertainty matrix \\(B\\) ; lumia.optimizer implements the optimization algorithm; lumia.data implements the emissions ( \\(E\\) ) themselves; lumia.mapping implements the mapping operator ( \\(M\\) ), that is used to convert between model data and control vector. lumia.utils contains various utilities, used by several of the other lumia modules. The Utilities page also describes the gridtools module, that is distributed along lumia . transport acts as a CTM ( \\(H\\) ) in our LUMIA inversions. Most of these modules can be used independently. For instance, the lumia.observations defines an Observation class that can be used to read, write and analyze observation files used in LUMIA (including LUMIA results). Example scripts showing a full inversion are provided under the run directory, and presented in more details in the Tutorial . Finally, some top-level objects (classes and functions) are described under the LUMIA overview page. The page also contains a more elaborate information on the coding strategy of LUMIA. LUMIA Development strategy LUMIA was designed as a modular system: it should remain as easy as possible to replace the default CTM and the individual components (prior, observations, optimizer, etc.) should remain as independent from each other as reasonable. In practice this means that the following technical choices were made: The LUMIA modules contain python classes to handle the different components of the inversions (e.g. an Observations class to handle the observations, a PriorConstraints class to construct and store the prior uncertainty matrix, etc.). All modules implement or support alternative versions of these classes. For instance, the \"Observations\" class is defined in the lumia.observations.observations class (i.e. lumia/observations/observations.py file): This approach makes space for alternative implementations (e.g. one could implement an Observations class part of a lumia.observations.satellite_obs module). It can however make imports paths length ( from lumia.observations.observations import Observations , from lumia.models.footprints.data import Data , etc.). To circumvent this, many \"shortcuts\" have been defined in the __init__.py files of the module (enabling e.g. from lumia.observations import Observations ), and, even in the top-level __init__.py file ( lumia/__init__.py ). These are documented in the relevant sections (e.g. in Observations ) and, for the top-level objects, in the section below. In several of the modules, there is a protocol.py file, which essentially contain templates for the various classes. In theory, as long as alternative classes follow the template defined in the protocol.py files, they should not lead to major compatibility issues. Although it was developed along LUMIA, the code that acts as a CTM ( transport ) remains a separate python package and is run as a subprocess: This forces us to limit the degree of inter-dependency between the two codes, and thus should make it easier implementing an alternative CTM.","title":"Documentation summary"},{"location":"data/","text":"Emissions The emissions to be transported are defined in the emissions section of the configuration file . The section is structured as follows: emissions : tracer : [tracer_i, tracer_j, ...] # List of tracers tracer_i: region : # Grid definition for tracer \"tracer_i\" interval : # transport model temporal resolution categories : cat1 : # \"origin\" of the emission files for cat \"cat1\" cat2 : origin : # \"origin\" of the emission files for cat \"cat2\" ... : # other settings, speficic to cat2 ... metacategories : metacat1 : metacat2 : ... : tracer_j: ... This section of the configuration file is essentially used to construct a single LUMIA emission file, containing the emissions for the simulation (i.e. emissions of all categories and all tracers, covering the entire length of the simulation), based on category and tracer-specific pre-processed emission files (see File formats section below). The yaml file keys determine the following settings: Mandatory keys: list of emission tracers: emissions.tracers key (should be a list!) path of the pre-processed files to be read: path , prefix and origin keys. grid definition of the emission files : emissions.{tracer}.region key temporal resolution of the emissions: emissions.{tracer}.interval key Optional keys: name of the netCDF variable to be read: field key whether the pre-processed emission files should be resampled from a different temporal resolution: resample_from key. path to a rclone-mounted emission archive: archive key definition of metacategories: emissions.{tracer}.metacategories key(s) The path , prefix , field , resample_from and archive keys can be provided at the tracer level (e.g. emissions.{tracer}.path ) or at the category level ( emissions.{tracer}.categories.{catname}.path ). The origin key should be provided at the category level ( emissions.{tracer}.categories.{catname}.origin ), but if all the other keys are provided at the tracer-level, it is possible to simply use emissions.{tracer}.categories.{catname} as an \"origin\" key. Pre-processed emission files The assembling of pre-processed, category-specific emission files into a single, simulation-specific emission file is performed by the lumia.Data.from_dconf method . The files should contain three coordinate variables ( time , lat and lon ), one (or more) emission fields defined on the same coordinates. It is also recommended to include a ( lat , lon ) area field for convenience, but this is not read or required by LUMIA. The lat and lon variables should refer to the center of the grid cells, while the time coordinate points to the start of each time period. The time coordinate should contain integer, and have a units and a calendar attribute, allowing conversion to a numpy.datetime64 type. The recommended method for creating these files is to use the xarray library. The path of the pre-processed files is determined by the path , prefix and origin keys: The file names follows the pattern {path}/{tres}/{prefix}{origin}.*.nc . Here prefix is meant as a tracer-spefic prefix (e.g. \"co2_flux\") and origin refers to the origin of the data in the file (e.g. \"LPJ\"). The {tres} key refers to the temporal resolution of the pre-processed emission files. By default it is identical as emissions.{tracer}.interval , but can be set at a lower temporal resolution using the resample_from key (in which case the emissions will simply be rebinned by LUMIA). For instance, if a biosphere category should read emissions from /data/LUMIA/M/co2_emis.LPJGUESS-v20.2018.nc , then: emissions.co2.path (or emissions.co2.categories.biosphere.path ) should be set to /data/LUMIA emissions.co2.prefix (or emissions.co2.categories.biosphere.prefix ) should be set to co2_emis. emissions.co2.categories.biosphere.origin (or emissions.co2.categories.biosphere ) should be set to LPJGUESS-v20 if emissions.co2.interval is not set to M , then a emissions.co2.resample_from (or emissions.co2.categories.biosphere.resample_from ) key should be defined and set to M . if the file contains any other variable than the coordinates ( lat , lon , time ) and the variable containing the emission themselves, then a emissions.co2.categories.biosphere.field variable should be set. Note that at no point we specify the time component of the filename: LUMIA will load all the files matching the pattern {path}/{prefix}{origin}.*.nc as a multi-file netCDF Dataset . Archive The emission files need to be on a local file system for LUMIA to read them, however that can be just a temporary folder, with the files being stored on a remote permanent storage, accessed via rclone . The path to the archive should be provided by the emissions.{tracer}.archive key, with the pattern rclone:{rclone_remote}:path/to/remote/dir , where {rclone_remote} is a rclone remote path defined in your rclone.conf file (see the relevant rclone documentation ). Meta-categories It is possible to define meta-categories , as a linear combination of other categories. For instance in the (fictional) example below, we defined one \"NEE\" meta-category, as the sum of the GPP and respiration categories. We defined a \"fossil\" category by subtraction of \"agri_fires\" from the \"anthropogenic\" category, and we created a \"fires\" category combining the \"natural_fires\" one and the \"agri_fires\" one, but the latter with a 1.2 scaling factor. By default, the meta-categories are not transported (i.e. the transport model is unaware of them), but their impact on the concentrations is calculated. On the other hand, the categories used to build the meta-categories are transported, but their impact on concentrations is ignored. emissions : co2 : categories: GPP : LPJ_GPP respiration : LPJ_resp anthropogenic : TNO natural_fires : GFAS agri_fires : origin : EDGAR field : agriwasteburning meta-categories : NEE : GPP + respiration fossil : anthropogenic - agri_fires fires : natural_fires + 1.2 * agri_fires LUMIA emission file raise SectionYetToBeWrittenError ;-)","title":"Emissions"},{"location":"data/#emissions","text":"The emissions to be transported are defined in the emissions section of the configuration file . The section is structured as follows: emissions : tracer : [tracer_i, tracer_j, ...] # List of tracers tracer_i: region : # Grid definition for tracer \"tracer_i\" interval : # transport model temporal resolution categories : cat1 : # \"origin\" of the emission files for cat \"cat1\" cat2 : origin : # \"origin\" of the emission files for cat \"cat2\" ... : # other settings, speficic to cat2 ... metacategories : metacat1 : metacat2 : ... : tracer_j: ... This section of the configuration file is essentially used to construct a single LUMIA emission file, containing the emissions for the simulation (i.e. emissions of all categories and all tracers, covering the entire length of the simulation), based on category and tracer-specific pre-processed emission files (see File formats section below). The yaml file keys determine the following settings: Mandatory keys: list of emission tracers: emissions.tracers key (should be a list!) path of the pre-processed files to be read: path , prefix and origin keys. grid definition of the emission files : emissions.{tracer}.region key temporal resolution of the emissions: emissions.{tracer}.interval key Optional keys: name of the netCDF variable to be read: field key whether the pre-processed emission files should be resampled from a different temporal resolution: resample_from key. path to a rclone-mounted emission archive: archive key definition of metacategories: emissions.{tracer}.metacategories key(s) The path , prefix , field , resample_from and archive keys can be provided at the tracer level (e.g. emissions.{tracer}.path ) or at the category level ( emissions.{tracer}.categories.{catname}.path ). The origin key should be provided at the category level ( emissions.{tracer}.categories.{catname}.origin ), but if all the other keys are provided at the tracer-level, it is possible to simply use emissions.{tracer}.categories.{catname} as an \"origin\" key.","title":"Emissions"},{"location":"data/#pre-processed-emission-files","text":"The assembling of pre-processed, category-specific emission files into a single, simulation-specific emission file is performed by the lumia.Data.from_dconf method . The files should contain three coordinate variables ( time , lat and lon ), one (or more) emission fields defined on the same coordinates. It is also recommended to include a ( lat , lon ) area field for convenience, but this is not read or required by LUMIA. The lat and lon variables should refer to the center of the grid cells, while the time coordinate points to the start of each time period. The time coordinate should contain integer, and have a units and a calendar attribute, allowing conversion to a numpy.datetime64 type. The recommended method for creating these files is to use the xarray library. The path of the pre-processed files is determined by the path , prefix and origin keys: The file names follows the pattern {path}/{tres}/{prefix}{origin}.*.nc . Here prefix is meant as a tracer-spefic prefix (e.g. \"co2_flux\") and origin refers to the origin of the data in the file (e.g. \"LPJ\"). The {tres} key refers to the temporal resolution of the pre-processed emission files. By default it is identical as emissions.{tracer}.interval , but can be set at a lower temporal resolution using the resample_from key (in which case the emissions will simply be rebinned by LUMIA). For instance, if a biosphere category should read emissions from /data/LUMIA/M/co2_emis.LPJGUESS-v20.2018.nc , then: emissions.co2.path (or emissions.co2.categories.biosphere.path ) should be set to /data/LUMIA emissions.co2.prefix (or emissions.co2.categories.biosphere.prefix ) should be set to co2_emis. emissions.co2.categories.biosphere.origin (or emissions.co2.categories.biosphere ) should be set to LPJGUESS-v20 if emissions.co2.interval is not set to M , then a emissions.co2.resample_from (or emissions.co2.categories.biosphere.resample_from ) key should be defined and set to M . if the file contains any other variable than the coordinates ( lat , lon , time ) and the variable containing the emission themselves, then a emissions.co2.categories.biosphere.field variable should be set. Note that at no point we specify the time component of the filename: LUMIA will load all the files matching the pattern {path}/{prefix}{origin}.*.nc as a multi-file netCDF Dataset .","title":"Pre-processed emission files"},{"location":"data/#archive","text":"The emission files need to be on a local file system for LUMIA to read them, however that can be just a temporary folder, with the files being stored on a remote permanent storage, accessed via rclone . The path to the archive should be provided by the emissions.{tracer}.archive key, with the pattern rclone:{rclone_remote}:path/to/remote/dir , where {rclone_remote} is a rclone remote path defined in your rclone.conf file (see the relevant rclone documentation ).","title":"Archive"},{"location":"data/#meta-categories","text":"It is possible to define meta-categories , as a linear combination of other categories. For instance in the (fictional) example below, we defined one \"NEE\" meta-category, as the sum of the GPP and respiration categories. We defined a \"fossil\" category by subtraction of \"agri_fires\" from the \"anthropogenic\" category, and we created a \"fires\" category combining the \"natural_fires\" one and the \"agri_fires\" one, but the latter with a 1.2 scaling factor. By default, the meta-categories are not transported (i.e. the transport model is unaware of them), but their impact on the concentrations is calculated. On the other hand, the categories used to build the meta-categories are transported, but their impact on concentrations is ignored. emissions : co2 : categories: GPP : LPJ_GPP respiration : LPJ_resp anthropogenic : TNO natural_fires : GFAS agri_fires : origin : EDGAR field : agriwasteburning meta-categories : NEE : GPP + respiration fossil : anthropogenic - agri_fires fires : natural_fires + 1.2 * agri_fires","title":"Meta-categories"},{"location":"data/#lumia-emission-file","text":"raise SectionYetToBeWrittenError ;-)","title":"LUMIA emission file"},{"location":"dummy/","text":"","title":"Dummy"},{"location":"observations/","text":"Observations The lumia.observations module provides classes to store data in the observations space (observation vector, observation uncertainties, observation coordinates, model estimates for concentrations, etc.). Modularity The design of the observations class depends, to some extent, of the type of observations (e.g. in-situ observations vs. satellite observations) and of the type of transport model (what metadata needs to be stored?). This document describes the default class ( lumia.Observations , a.k.a. lumia.observations.observations.Observations ). It is designed for handling in-situ data, and for use with the default lumia.Transport class. Structure The Observations object contains two main data attributes: sites ( pandas.DataFrame ) contains information on observation series: site name and code, coordinates (if relevant), observation type, data provider, DOI, etc. observations ( pandas.DataFrame ) contains information on the observation themselves: observed concentrations, estimated uncertainties, coordinates, etc.) The Observations class also implements a few methods: I/O methods, to read and write the observations to various file formats (HDF, tar.gz) selection methods, to refine the data selection and extract a subset of the observations. calc_uncertainty : a method to estimate the observation uncertainties (i.e. combine the measurement uncertainty with an estimate of the model uncertainty). Finally, the Observation objects have two \"properties\" (attributes): mismatch and sigma , which contain respectively the model-data mismatches (model - obs) and the uncertainty (1 \\(\\sigma\\) ) on them. These attributes are accessed by the optimizer. Construction The simplest way to create a new Observations object is to construct first its sites and observations tables: The Observations.sites table contain information common to a whole observation series. For instance, the site name and coordinates, sitecode, tracer, units, data provider, DOI, source file, etc. There is no specific or mandatory list of columns, the table is here for the user convenience. The Observations.observations table contains the observations themselves, their uncertainties, relevant metadata (time, sampling height, etc.) and more generally any data in the observation space that may be required by the model (e.g. path to pre-computed observation footprints) or that it may be worth storing. It is also where modelled values will be stored. The observations table has a few mandatory or recommended columns: obs : the observed concentrations err : the total observation uncertainty (i.e. uncertainty on the model-data mismatch, including both the observation uncertainty and the model uncertainty) time : the observation time height : the sampling height (above ground) of the observations code : the site code site : the index of the relevant row in the Observations.sites table mix_background : the background concentrations (i.e. boundary condition). One way to see it is that the \"observations\" table stores what is usually the data part of an observation files (i.e. the columns in a CSV file, or the variables in a netCDF file), whereas the \"sites\" table stores the metadata (header in a CSV file, attributes in a netCDF file). The code below shows a partial example for importing a bunch of files (in this example, ICOS CSV files) to a lumia.Observations object, and finally storing that object as a tar.gz file: from pathlib import Path from pandas import DataFrame, read_csv, concat from lumia import Observations def import_csv_icos(filename : Path) -> (dict, DataFrame): \"\"\" A function that reads one ICOS observation file (in CSV format), and returns: - metadata: a dictionary containing the elements that should go in the `Observations.sites` table - data: a DataFrame containing the section of the `Observations.observations` table corresponding to this file \"\"\" ... return metadata, data # Create temporary lists to store the imported data sites, observations = [], [] # Loop over the files (look for every file matching the \"/path/to/observations/co2*.csv\" pattern for filepath in Path('/path/to/observations').glob('ICOS_ATC_*.CO2'): site, obs = import_csv_icos(filepath) sites.append(site) observations.append(obs) # Concatenate the sites and observations lists in two DataFrames: sites = concat(sites) observations = concat(observations) # Create the actual lumia.Observations object obs = Observations(sites=sites, observations=observations) # Store it to disk: obs.save_tar('observations.tar.gz') I/O and file formats The Observations class can read and write to two different file formats: tar.gz files are the default file format. The sites and observations tables are simply written out as CSV files, and clobbed together in a gzip-compressed tar files. The main advantage of that format is that it remains human-readable (one only needs to uncompress the tar.gz file). On the other hand, it is not very fast. hdf files are also used internally, for communication with the transport package . The tar.gz format is the recommended one for day-to-day interactions: import lumia # Read an observation file in tar.gz format: obs = lumia.Observations.from_tar('observations.tar.gz') # Write an Observations object to a file: obs.save_tar('observations.tar.gz') Other methods: The list may not be complete: refer to the actual code for further information. Observations.calc_uncertainties : Compute an estimate of the uncertainties in the observation space, combining the measurement uncertainty and model uncertainty. The method is called at the end of the first forward iteration, as it (may) need the prior model estimates for the observations. See documentation in the code for further details. The method may need an additional settings attribute to be defined. Observations.select_times : when provided with a list of sites (indices of the Observations.sites table), returns a new Observations object, containing only data for these sites. Observations.select_sites : returns a new Observations object, limited to the provided range of dates.","title":"Observations"},{"location":"observations/#observations","text":"The lumia.observations module provides classes to store data in the observations space (observation vector, observation uncertainties, observation coordinates, model estimates for concentrations, etc.). Modularity The design of the observations class depends, to some extent, of the type of observations (e.g. in-situ observations vs. satellite observations) and of the type of transport model (what metadata needs to be stored?). This document describes the default class ( lumia.Observations , a.k.a. lumia.observations.observations.Observations ). It is designed for handling in-situ data, and for use with the default lumia.Transport class.","title":"Observations"},{"location":"observations/#structure","text":"The Observations object contains two main data attributes: sites ( pandas.DataFrame ) contains information on observation series: site name and code, coordinates (if relevant), observation type, data provider, DOI, etc. observations ( pandas.DataFrame ) contains information on the observation themselves: observed concentrations, estimated uncertainties, coordinates, etc.) The Observations class also implements a few methods: I/O methods, to read and write the observations to various file formats (HDF, tar.gz) selection methods, to refine the data selection and extract a subset of the observations. calc_uncertainty : a method to estimate the observation uncertainties (i.e. combine the measurement uncertainty with an estimate of the model uncertainty). Finally, the Observation objects have two \"properties\" (attributes): mismatch and sigma , which contain respectively the model-data mismatches (model - obs) and the uncertainty (1 \\(\\sigma\\) ) on them. These attributes are accessed by the optimizer.","title":"Structure"},{"location":"observations/#construction","text":"The simplest way to create a new Observations object is to construct first its sites and observations tables: The Observations.sites table contain information common to a whole observation series. For instance, the site name and coordinates, sitecode, tracer, units, data provider, DOI, source file, etc. There is no specific or mandatory list of columns, the table is here for the user convenience. The Observations.observations table contains the observations themselves, their uncertainties, relevant metadata (time, sampling height, etc.) and more generally any data in the observation space that may be required by the model (e.g. path to pre-computed observation footprints) or that it may be worth storing. It is also where modelled values will be stored. The observations table has a few mandatory or recommended columns: obs : the observed concentrations err : the total observation uncertainty (i.e. uncertainty on the model-data mismatch, including both the observation uncertainty and the model uncertainty) time : the observation time height : the sampling height (above ground) of the observations code : the site code site : the index of the relevant row in the Observations.sites table mix_background : the background concentrations (i.e. boundary condition). One way to see it is that the \"observations\" table stores what is usually the data part of an observation files (i.e. the columns in a CSV file, or the variables in a netCDF file), whereas the \"sites\" table stores the metadata (header in a CSV file, attributes in a netCDF file). The code below shows a partial example for importing a bunch of files (in this example, ICOS CSV files) to a lumia.Observations object, and finally storing that object as a tar.gz file: from pathlib import Path from pandas import DataFrame, read_csv, concat from lumia import Observations def import_csv_icos(filename : Path) -> (dict, DataFrame): \"\"\" A function that reads one ICOS observation file (in CSV format), and returns: - metadata: a dictionary containing the elements that should go in the `Observations.sites` table - data: a DataFrame containing the section of the `Observations.observations` table corresponding to this file \"\"\" ... return metadata, data # Create temporary lists to store the imported data sites, observations = [], [] # Loop over the files (look for every file matching the \"/path/to/observations/co2*.csv\" pattern for filepath in Path('/path/to/observations').glob('ICOS_ATC_*.CO2'): site, obs = import_csv_icos(filepath) sites.append(site) observations.append(obs) # Concatenate the sites and observations lists in two DataFrames: sites = concat(sites) observations = concat(observations) # Create the actual lumia.Observations object obs = Observations(sites=sites, observations=observations) # Store it to disk: obs.save_tar('observations.tar.gz')","title":"Construction"},{"location":"observations/#io-and-file-formats","text":"The Observations class can read and write to two different file formats: tar.gz files are the default file format. The sites and observations tables are simply written out as CSV files, and clobbed together in a gzip-compressed tar files. The main advantage of that format is that it remains human-readable (one only needs to uncompress the tar.gz file). On the other hand, it is not very fast. hdf files are also used internally, for communication with the transport package . The tar.gz format is the recommended one for day-to-day interactions: import lumia # Read an observation file in tar.gz format: obs = lumia.Observations.from_tar('observations.tar.gz') # Write an Observations object to a file: obs.save_tar('observations.tar.gz')","title":"I/O and file formats"},{"location":"observations/#other-methods","text":"The list may not be complete: refer to the actual code for further information. Observations.calc_uncertainties : Compute an estimate of the uncertainties in the observation space, combining the measurement uncertainty and model uncertainty. The method is called at the end of the first forward iteration, as it (may) need the prior model estimates for the observations. See documentation in the code for further details. The method may need an additional settings attribute to be defined. Observations.select_times : when provided with a list of sites (indices of the Observations.sites table), returns a new Observations object, containing only data for these sites. Observations.select_sites : returns a new Observations object, limited to the provided range of dates.","title":"Other methods:"},{"location":"prior/","text":"Prior uncertainties The uncertainty settings should be grouped in a section of the configuration file, with the following structure : optimize : emissions : tracer1 : category1 : annual_uncertainty : spatial_correlation : cortype : temporal_correlation : optimization_interval : category2 : ... tracer2 : ... The annual_uncertainty settings should be in the form of a value and unit (e.g. \"1.2 TgCH4\", \"0.5 PgCO2\", ...). The optimization_interval settings should be a string understandable by the pandas.tseries.frequencies.to_offset function. The spatial_correlation and temporal_correlation should contain a cortype sub-key, which determines which correlation function should be used. Depending on this, they may require additional subkeys (e.g. corlen , stretch_ratio , etc.). Error-covariance matrix The error covariance matrix is calculated in three steps: The standard are estimated, for each state vector component The correlations are determned The resulting total uncertainty is computed (by combining the standard deviations and correlations), and the standard deviations are scaled uniformly, to match a user-specified annual uncertainty target. where is it in the code? The uncertainty matrix is calculated in the lumia.PriorConstraints.setup method, which should be called by the main script (see, e.g. run/co2_inversion.py ) 1. Standard deviations The standard deviations ( \\(\\sigma_\\mathbf{x}\\) , i.e. the diagonal elements of the \\(\\mathbf{B}\\) matrix) are prescribed based on the absolute value of the prior emissions. Consider a control variable \\(x\\) , controlling for the offsets to the prior emissions for a group of contiguous grid cells \\(p_i\\) to \\(p_j\\) , and from time steps \\(t_n\\) to \\(t_m\\) . Then the value of \\(x\\) in a given iteration is: \\[ x = \\sum_{t = n}^{m}\\sum_{p = i}^{j} \\left(E(t, p) - E^{apri}(t, p)\\right)\\] The prior values of \\(x\\) (i.e. when \\(E = E^{apri}\\) ) is therefore 0. The corresponding prior uncertainty, \\(\\sigma_x\\) , is defined as: \\[\\sigma_x = \\sum_{t = n}^m\\sum_{p = i}^j |E^{apri}(t, p)|\\] In other words, the uncertainty on \\(x\\) is proportional to the sum of the absolute values of the \"components\" of \\(x\\) . This ensures in particular that the uncertainty doesn't drop to zero if the fluxes to which \\(x\\) is an offset happen to cancel out each other (e.g. when respiration and photosynthesis are of equal amplitude). Note that the absolute values of the standard deviations computed in this step has no importance, as they are scaled to match a target annual uncertainty, in step 3 below. The aim of this step 1 is only to define how this annual uncertainty will be distributed, in time and space. where is it in the code? The standard deviations are directly calculated in the lumia.PriorConstraints.setup method. It relies on the lumia.Mapping.coarsen_cat method to aggregate the model-resolution error estimates into a control-vector-like object. 2. Correlations Four correlation methods are implemented, and can be selected using the cortype subkey of the spatial_correlation and temporal_correlation sections. Three of them calculate correlation coefficients based on a mathematical function of the spatial or temporal distance between the grid-cells, whereas the last one reads pre-computed correlation-distance relationships from a file. where is it in the code? The uncertainty settings are read within the lumia.Mapping.setup_optimization method. In addition, if you need to implement new options, you might need to edit the lumia.optimizer.categories.Categories class. Correlation functions Exponential ( cortype = e): \\(r = e^{- \\left(d / L\\right)}\\) Gaussian ( cortype = g): \\(r_H = e^{- \\left(d / L\\right)^2}\\) Hyperbolic ( cortype = h): \\(r_H = 1 / \\left(1 + d / L\\right)\\) where \\(d\\) is the geographical distance between the center of two gridcells, and \\(L\\) is a user-specified correlation length ( optimize.emissions.{tracer}.{catname}.horizontal_correlation.cortype key) For temporal correlations, only the exponential method is implemented, and cortype should be a string understandable by the pandas.tseries.frequencies.to_offset function. The last method ( cortype = f) reads the correlation-distance relationship ( \\(r(d)\\) ) from a file (see below). In which case, a corrfile argument is needed (and corlen is not used). where is it in the code? The correlation functions are all implemented in the lumia.prior.uncertainties module ( SpatialCorrelation and TemporalCorrelation classes). Correlation file Implementation pending It is possible to provide correlation-distance relationships through a file. This allows for more complex correlation functions to be implemented. In order to use this feature, set cortype to \"f\" (or to \"file\"), and provide the name of the correlation file in a corfile key. The correlation file should be a netCDF file, containing one correlations group, with the following structure: group: correlations { dimensions: point = 1644 ; time = 365 ; variables: double lat(point) ; lat:_FillValue = NaN ; double lon(point) ; lon:_FillValue = NaN ; double horizontal_correlation(point, point) ; horizontal_correlation:_FillValue = NaN ; double temporal_correlations(time, time) ; temporal_correlations:_FillValue = NaN ; int64 time(time) ; time:units = \"days since 2018-01-01 00:00:00\" ; time:calendar = \"proleptic_gregorian\" ; int64 point(point) ; } // group correlations The horizontal_correlations and temporal_correlations variables contain the spatial and temporal matrices. The time variable is the coordinate of the time dimension of the temporal_correlations matrix. The lat and lon variables store the spatial positions corresponding to the point coordinate of the horizontal_correlation matrix. where is it in the code? The matrices are read within the SpatialCorrelation and TemporalCorrelation classes of the lumia.prior.uncertainties module. Specifically, it uses the read_spatial_correlations and read_temporal_correlations functions of that module, which doesn't just read the matrices, but also ensure that they are compatible with the current list of coordinates, since there can be nan values in the pre-computed matrices. 3. Uncertainty scaling The net uncertainty is calculated as: \\[ \\sigma_{tot} = \\sqrt{\\mathbf{s} \\cdot vec(\\mathbf{C_h S C_t}^T)} \\] with \\(s\\) the vector containing the standard deviations of \\(x\\) , \\(S\\) the same vector, but reshaped as a ( nt , np ) matrix (with nt and np respectively the number of optimization time steps and optimization (clusters of) grid cells), and vec the operator reshaping a ( nt , np ) matrix as a np \\(\\times\\) nt vector. \\(\\mathbf{C_t}\\) and \\(\\mathbf{C_h}\\) are the temporal and spatial correlation matrices calculated in step 2. This relies on the properties that: \\(\\mathbf{B = C_t \\otimes C_h}\\) the sum of a covariance matrix can be inferred from \\(\\mathbf{s \\cdot Q \\cdot s^T}\\) (with \\(\\mathbf{s}\\) the vector of standard deviations, and \\(\\mathbf{Q}\\) the correlation matrix The equivalence between \\((\\mathbf{C_t \\otimes C_h)} vec(\\mathbf{S})\\) and \\(vec(\\mathbf{C_h S C_t^T})\\) Combining these three equations, we obtain \\(\\mathbf{\\Sigma^2} = \\mathbf{s \\cdot C_t \\otimes C_h \\cdot s^T = s} \\cdot vec(\\mathbf{C_h S C_t^T})^T\\) , with \\(\\mathbf{\\Sigma^2}\\) the total variance. Because the formula above doesn't construct the covariance matrix explicitly, but only its sum, it is (relatively) lightweight and efficient to compute. Once the total variance is computed, the standard deviations are multiplied by a scalar factor \\(\\gamma\\) , defined as \\[ \\gamma = \\frac{\\mathbf{\\Sigma_{target}}}{\\mathbf{\\Sigma}}\\frac{\\Delta}{\\Delta_y} \\] with \\(\\Sigma_{target}\\) the target annual uncertainty, \\(\\Delta\\) the length of the simulation (in seconds) and \\(\\Delta_y\\) the lenght of one year. where is it in the code? The uncertainty scaling is done directly in the lumia.PriorConstraints.setup method.","title":"Prior uncertainties"},{"location":"prior/#prior-uncertainties","text":"The uncertainty settings should be grouped in a section of the configuration file, with the following structure : optimize : emissions : tracer1 : category1 : annual_uncertainty : spatial_correlation : cortype : temporal_correlation : optimization_interval : category2 : ... tracer2 : ... The annual_uncertainty settings should be in the form of a value and unit (e.g. \"1.2 TgCH4\", \"0.5 PgCO2\", ...). The optimization_interval settings should be a string understandable by the pandas.tseries.frequencies.to_offset function. The spatial_correlation and temporal_correlation should contain a cortype sub-key, which determines which correlation function should be used. Depending on this, they may require additional subkeys (e.g. corlen , stretch_ratio , etc.).","title":"Prior uncertainties"},{"location":"prior/#error-covariance-matrix","text":"The error covariance matrix is calculated in three steps: The standard are estimated, for each state vector component The correlations are determned The resulting total uncertainty is computed (by combining the standard deviations and correlations), and the standard deviations are scaled uniformly, to match a user-specified annual uncertainty target. where is it in the code? The uncertainty matrix is calculated in the lumia.PriorConstraints.setup method, which should be called by the main script (see, e.g. run/co2_inversion.py )","title":"Error-covariance matrix"},{"location":"prior/#1-standard-deviations","text":"The standard deviations ( \\(\\sigma_\\mathbf{x}\\) , i.e. the diagonal elements of the \\(\\mathbf{B}\\) matrix) are prescribed based on the absolute value of the prior emissions. Consider a control variable \\(x\\) , controlling for the offsets to the prior emissions for a group of contiguous grid cells \\(p_i\\) to \\(p_j\\) , and from time steps \\(t_n\\) to \\(t_m\\) . Then the value of \\(x\\) in a given iteration is: \\[ x = \\sum_{t = n}^{m}\\sum_{p = i}^{j} \\left(E(t, p) - E^{apri}(t, p)\\right)\\] The prior values of \\(x\\) (i.e. when \\(E = E^{apri}\\) ) is therefore 0. The corresponding prior uncertainty, \\(\\sigma_x\\) , is defined as: \\[\\sigma_x = \\sum_{t = n}^m\\sum_{p = i}^j |E^{apri}(t, p)|\\] In other words, the uncertainty on \\(x\\) is proportional to the sum of the absolute values of the \"components\" of \\(x\\) . This ensures in particular that the uncertainty doesn't drop to zero if the fluxes to which \\(x\\) is an offset happen to cancel out each other (e.g. when respiration and photosynthesis are of equal amplitude). Note that the absolute values of the standard deviations computed in this step has no importance, as they are scaled to match a target annual uncertainty, in step 3 below. The aim of this step 1 is only to define how this annual uncertainty will be distributed, in time and space. where is it in the code? The standard deviations are directly calculated in the lumia.PriorConstraints.setup method. It relies on the lumia.Mapping.coarsen_cat method to aggregate the model-resolution error estimates into a control-vector-like object.","title":"1. Standard deviations"},{"location":"prior/#2-correlations","text":"Four correlation methods are implemented, and can be selected using the cortype subkey of the spatial_correlation and temporal_correlation sections. Three of them calculate correlation coefficients based on a mathematical function of the spatial or temporal distance between the grid-cells, whereas the last one reads pre-computed correlation-distance relationships from a file. where is it in the code? The uncertainty settings are read within the lumia.Mapping.setup_optimization method. In addition, if you need to implement new options, you might need to edit the lumia.optimizer.categories.Categories class.","title":"2. Correlations"},{"location":"prior/#correlation-functions","text":"Exponential ( cortype = e): \\(r = e^{- \\left(d / L\\right)}\\) Gaussian ( cortype = g): \\(r_H = e^{- \\left(d / L\\right)^2}\\) Hyperbolic ( cortype = h): \\(r_H = 1 / \\left(1 + d / L\\right)\\) where \\(d\\) is the geographical distance between the center of two gridcells, and \\(L\\) is a user-specified correlation length ( optimize.emissions.{tracer}.{catname}.horizontal_correlation.cortype key) For temporal correlations, only the exponential method is implemented, and cortype should be a string understandable by the pandas.tseries.frequencies.to_offset function. The last method ( cortype = f) reads the correlation-distance relationship ( \\(r(d)\\) ) from a file (see below). In which case, a corrfile argument is needed (and corlen is not used). where is it in the code? The correlation functions are all implemented in the lumia.prior.uncertainties module ( SpatialCorrelation and TemporalCorrelation classes).","title":"Correlation functions"},{"location":"prior/#correlation-file","text":"Implementation pending It is possible to provide correlation-distance relationships through a file. This allows for more complex correlation functions to be implemented. In order to use this feature, set cortype to \"f\" (or to \"file\"), and provide the name of the correlation file in a corfile key. The correlation file should be a netCDF file, containing one correlations group, with the following structure: group: correlations { dimensions: point = 1644 ; time = 365 ; variables: double lat(point) ; lat:_FillValue = NaN ; double lon(point) ; lon:_FillValue = NaN ; double horizontal_correlation(point, point) ; horizontal_correlation:_FillValue = NaN ; double temporal_correlations(time, time) ; temporal_correlations:_FillValue = NaN ; int64 time(time) ; time:units = \"days since 2018-01-01 00:00:00\" ; time:calendar = \"proleptic_gregorian\" ; int64 point(point) ; } // group correlations The horizontal_correlations and temporal_correlations variables contain the spatial and temporal matrices. The time variable is the coordinate of the time dimension of the temporal_correlations matrix. The lat and lon variables store the spatial positions corresponding to the point coordinate of the horizontal_correlation matrix. where is it in the code? The matrices are read within the SpatialCorrelation and TemporalCorrelation classes of the lumia.prior.uncertainties module. Specifically, it uses the read_spatial_correlations and read_temporal_correlations functions of that module, which doesn't just read the matrices, but also ensure that they are compatible with the current list of coordinates, since there can be nan values in the pre-computed matrices.","title":"Correlation file"},{"location":"prior/#3-uncertainty-scaling","text":"The net uncertainty is calculated as: \\[ \\sigma_{tot} = \\sqrt{\\mathbf{s} \\cdot vec(\\mathbf{C_h S C_t}^T)} \\] with \\(s\\) the vector containing the standard deviations of \\(x\\) , \\(S\\) the same vector, but reshaped as a ( nt , np ) matrix (with nt and np respectively the number of optimization time steps and optimization (clusters of) grid cells), and vec the operator reshaping a ( nt , np ) matrix as a np \\(\\times\\) nt vector. \\(\\mathbf{C_t}\\) and \\(\\mathbf{C_h}\\) are the temporal and spatial correlation matrices calculated in step 2. This relies on the properties that: \\(\\mathbf{B = C_t \\otimes C_h}\\) the sum of a covariance matrix can be inferred from \\(\\mathbf{s \\cdot Q \\cdot s^T}\\) (with \\(\\mathbf{s}\\) the vector of standard deviations, and \\(\\mathbf{Q}\\) the correlation matrix The equivalence between \\((\\mathbf{C_t \\otimes C_h)} vec(\\mathbf{S})\\) and \\(vec(\\mathbf{C_h S C_t^T})\\) Combining these three equations, we obtain \\(\\mathbf{\\Sigma^2} = \\mathbf{s \\cdot C_t \\otimes C_h \\cdot s^T = s} \\cdot vec(\\mathbf{C_h S C_t^T})^T\\) , with \\(\\mathbf{\\Sigma^2}\\) the total variance. Because the formula above doesn't construct the covariance matrix explicitly, but only its sum, it is (relatively) lightweight and efficient to compute. Once the total variance is computed, the standard deviations are multiplied by a scalar factor \\(\\gamma\\) , defined as \\[ \\gamma = \\frac{\\mathbf{\\Sigma_{target}}}{\\mathbf{\\Sigma}}\\frac{\\Delta}{\\Delta_y} \\] with \\(\\Sigma_{target}\\) the target annual uncertainty, \\(\\Delta\\) the length of the simulation (in seconds) and \\(\\Delta_y\\) the lenght of one year. where is it in the code? The uncertainty scaling is done directly in the lumia.PriorConstraints.setup method.","title":"3. Uncertainty scaling"},{"location":"settings/","text":"Configuration file The settings are contained in a configuration file in yaml format. The file contains six major sections: the run section, which contains general simulation settings (domain, resolution, etc.) the emissions section contains all settings needed to construct the file containing the surface fluxes the observations section contains all settings needed to handle the observations (obs file, units, uncertainties, etc.) the optimize section contains settings specific to the inversion (which category needs to be optimized, with which uncertainties, etc.) the model section contains settings of the transport model the minimizer section contains settings of the conjugate gradient minimizer Additional sections may be defined for convenience. The values in this page are those used for the tutorial run section General settings (simulation extent and resolution, paths, etc.) run: start : 2018-01-01 end : 2019-01-01 timestep : 1h domain : eurocom025x025 grid : ${Grid:{lon0:-15, lat0:33, lon1:35, lat1:73, dlon:.5, dlat:.5}} tracers : co2 paths : data : data output : output footprints : footprints temp : temp start and end (time boundaries of the simulation) can be in any format supported by pandas.Timestamp . timestep can be anything supported by pandas.tseries.frequencies.to_offset . The grid key defines an instance of gridtools.Grid . The syntax is ${Grid:value} , with value a python dictionary containing keywords passed to gridtools.Grid . emissions section: The section contains keys needed to construct the emissions file. There needs to be one subsection for each tracer (only one \"co2\" tracer in this example), and an emissions.tracers key is also necessary. The emission file for the simulation is constructed from annual, category-specific, pre-processed emission files. The keys define the naming pattern for these files: the files are in data/fluxes/nc/eurocom025x025/1h the files for category biosphere are named following the pattern flux_co2.EDGARv4.3_BP2019.%Y.nc emissions : tracers : ${run.tracers} co2 : region : ${run.grid} interval : ${run.timestep} categories : fossil : EDGARv4.3_BP2019 biosphere : LPJ-GUESS_verify prefix : flux_co2. path : ${run.paths.data}/fluxes/nc/${run.domain}/{run.timestep}/ observations section: The section contains keys needed to read and process the observation database. In this tutorial, it just points to the right observation file, but further settings are possible, to restrict the time period, setup the uncertainties, etc. These settings are read in two places: directly in the relevant obsdb module ( lumia.obsdb.InversionDb.obsdb class) in a pre-processing step, e.g. in the lumia.ui.setup_observations method. See the in-line documentation of these two modules for further info observations : file : path : doc/observations/obs_example.tar.gz optimize section This section contains the keys that define the inversion problem. It contains two large subsection: one \"emissions\" subsection, which contains settings related to the state vector and its uncertainty matrix (structure of the uncertainty, number of optimized categories, resolution of the optimization, etc.) one \"observations\" subsection, which defines the way observation uncertainties are treated: optimize : emissions : co2 : biosphere : annual_uncertainty : 0.45 PgC spatial_correlation : 500-g temporal_correlation : 30D npoints : 2500 optimization_interval : 7d observations : co2 : uncertainty : type : dyn freq : 7d Here, the emissions will be optimized for the biosphere category of the co2 tracer. The total uncertainty is set to 0.45 PgC ( annual_uncertainty ), with correlations decaying spatially ( spatial_correlation ) following a 500 km Gaussian function, and temporally ( temporal_correlation ) following a 30 days exponential function (see sect 3.5.1 in https://gmd.copernicus.org/articles/14/3383/2021/ ). The inversion solves for 2500 cluster of pixels ( npoints ) each seven days ( optimization_interval ). With observations.co2.uncertainty.type set to dyn , the observation uncertainties are determined based on the quality of the fit of the short term (< 7 days) variability of the modelled concentration to the observed concentrations (see setup_uncertainties method). congrad section: This short section contains settings for the conjugate gradient minimizer. The main relevant user-setting is max_number_of_iterations (set to a lower value to speed things up, set to > 50 for scientific results): congrad : max_number_of_iterations : 80 communication_file : ${run.paths.temp}/congrad.nc executable : ${lumia:src/congrad/congrad.exe} model section This section contains settings for the interface between lumia and the transport model (i.e. lumia/obsoperator ) model : transport : exec : ${lumia:transport/multitracer.py} serial : False output : steps : ['apri', 'apos'] path : ${run.paths} Note the syntax of the model.transport.exec key: ${lumia:path} points to a path relative to the installation path of the lumia python module.","title":"Configuration file"},{"location":"settings/#configuration-file","text":"The settings are contained in a configuration file in yaml format. The file contains six major sections: the run section, which contains general simulation settings (domain, resolution, etc.) the emissions section contains all settings needed to construct the file containing the surface fluxes the observations section contains all settings needed to handle the observations (obs file, units, uncertainties, etc.) the optimize section contains settings specific to the inversion (which category needs to be optimized, with which uncertainties, etc.) the model section contains settings of the transport model the minimizer section contains settings of the conjugate gradient minimizer Additional sections may be defined for convenience. The values in this page are those used for the tutorial","title":"Configuration file"},{"location":"settings/#run-section","text":"General settings (simulation extent and resolution, paths, etc.) run: start : 2018-01-01 end : 2019-01-01 timestep : 1h domain : eurocom025x025 grid : ${Grid:{lon0:-15, lat0:33, lon1:35, lat1:73, dlon:.5, dlat:.5}} tracers : co2 paths : data : data output : output footprints : footprints temp : temp start and end (time boundaries of the simulation) can be in any format supported by pandas.Timestamp . timestep can be anything supported by pandas.tseries.frequencies.to_offset . The grid key defines an instance of gridtools.Grid . The syntax is ${Grid:value} , with value a python dictionary containing keywords passed to gridtools.Grid .","title":"run section"},{"location":"settings/#emissions-section","text":"The section contains keys needed to construct the emissions file. There needs to be one subsection for each tracer (only one \"co2\" tracer in this example), and an emissions.tracers key is also necessary. The emission file for the simulation is constructed from annual, category-specific, pre-processed emission files. The keys define the naming pattern for these files: the files are in data/fluxes/nc/eurocom025x025/1h the files for category biosphere are named following the pattern flux_co2.EDGARv4.3_BP2019.%Y.nc emissions : tracers : ${run.tracers} co2 : region : ${run.grid} interval : ${run.timestep} categories : fossil : EDGARv4.3_BP2019 biosphere : LPJ-GUESS_verify prefix : flux_co2. path : ${run.paths.data}/fluxes/nc/${run.domain}/{run.timestep}/","title":"emissions section:"},{"location":"settings/#observations-section","text":"The section contains keys needed to read and process the observation database. In this tutorial, it just points to the right observation file, but further settings are possible, to restrict the time period, setup the uncertainties, etc. These settings are read in two places: directly in the relevant obsdb module ( lumia.obsdb.InversionDb.obsdb class) in a pre-processing step, e.g. in the lumia.ui.setup_observations method. See the in-line documentation of these two modules for further info observations : file : path : doc/observations/obs_example.tar.gz","title":"observations section:"},{"location":"settings/#optimize-section","text":"This section contains the keys that define the inversion problem. It contains two large subsection: one \"emissions\" subsection, which contains settings related to the state vector and its uncertainty matrix (structure of the uncertainty, number of optimized categories, resolution of the optimization, etc.) one \"observations\" subsection, which defines the way observation uncertainties are treated: optimize : emissions : co2 : biosphere : annual_uncertainty : 0.45 PgC spatial_correlation : 500-g temporal_correlation : 30D npoints : 2500 optimization_interval : 7d observations : co2 : uncertainty : type : dyn freq : 7d Here, the emissions will be optimized for the biosphere category of the co2 tracer. The total uncertainty is set to 0.45 PgC ( annual_uncertainty ), with correlations decaying spatially ( spatial_correlation ) following a 500 km Gaussian function, and temporally ( temporal_correlation ) following a 30 days exponential function (see sect 3.5.1 in https://gmd.copernicus.org/articles/14/3383/2021/ ). The inversion solves for 2500 cluster of pixels ( npoints ) each seven days ( optimization_interval ). With observations.co2.uncertainty.type set to dyn , the observation uncertainties are determined based on the quality of the fit of the short term (< 7 days) variability of the modelled concentration to the observed concentrations (see setup_uncertainties method).","title":"optimize section"},{"location":"settings/#congrad-section","text":"This short section contains settings for the conjugate gradient minimizer. The main relevant user-setting is max_number_of_iterations (set to a lower value to speed things up, set to > 50 for scientific results): congrad : max_number_of_iterations : 80 communication_file : ${run.paths.temp}/congrad.nc executable : ${lumia:src/congrad/congrad.exe}","title":"congrad section:"},{"location":"settings/#model-section","text":"This section contains settings for the interface between lumia and the transport model (i.e. lumia/obsoperator ) model : transport : exec : ${lumia:transport/multitracer.py} serial : False output : steps : ['apri', 'apos'] path : ${run.paths} Note the syntax of the model.transport.exec key: ${lumia:path} points to a path relative to the installation path of the lumia python module.","title":"model section"},{"location":"tutorial/","text":"Step-by-step inversion tutorial This tutorial shows how to run a simple CO2 inversion, using example data (download from the ICOS Carbon Portal ). This assumes that: LUMIA has been installed The example observation file (obs_example.tgz) and configuration file (inversion.yaml) are present in the current folder Footprint files are present on disk, in the ./footprints folder Pre-processed emission files are present in the ./data/fluxes/eurocom025x025/1h folder If the data is in a different place, edit the config file (inversion.yaml). The step-by-step procedure described below is the equivalent of just running lumia optim --rc inversion.yaml . Import modules # Modules required by the inversion from lumia import ui # user-interface (higher-level methods) from lumia.config import RcFile # Config files from lumia.formatters import xr # emission files from lumia.interfaces.multitracer import Interface # Interface betwen the model state (gridded fluxes) and the optimization (state vector) import lumia # Modules required for display purpose in this notebook: from IPython.display import display from matplotlib.pyplot import subplots import cartopy from numpy import log from numpy.random import randint Load the configuration file Detailed description of the config file can be found in the Settings section. rcf = RcFile('inversion.yaml') Load the observations file. The observation file contains two pandas dataframes: the observations DataFrame contains the observation themselves. Mandatory columns are time, site, height, obs, err_obs, mix_background, code and tracer: the sites DataFrame contains the information that is common to all obs of one site (lat, lon, alt, site name and site code). There not really any mandatory column ... In the observations table, the site column contains indices of the sites table (leftermost column in the output below), while the code column contains the site codes (they are similar in this instance, but site could be anything). height is the sampling height (above ground), while alt is the ground altitude (above sea level) at the sites. err_obs is the measurement uncertainty. Use a value below 0 if unavailable. obs = ui.load_observations(rcf) display(obs.observations) display(obs.sites) 2023-01-04 16:55:51.852 | INFO | lumia.obsdb:load_tar:169 - 71907 observation read from obs_example.tgz .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } time site height obs err_obs mix_background code tracer 0 2018-01-03 23:00:00 ssl 12.0 411.95 -9.990 410.626136 ssl co2 1 2018-01-04 00:00:00 ssl 12.0 412.03 -9.990 410.596810 ssl co2 2 2018-01-04 01:00:00 ssl 12.0 412.04 -9.990 410.560833 ssl co2 3 2018-01-04 02:00:00 ssl 12.0 411.59 -9.990 410.515688 ssl co2 4 2018-01-04 03:00:00 ssl 12.0 411.73 -9.990 410.463696 ssl co2 ... ... ... ... ... ... ... ... ... 71902 2018-08-28 12:00:00 gic 20.0 402.39 0.222 402.745598 gic co2 71903 2018-08-28 13:00:00 gic 20.0 402.42 0.285 402.983801 gic co2 71904 2018-08-28 14:00:00 gic 20.0 402.61 0.438 403.182289 gic co2 71905 2018-08-28 15:00:00 gic 20.0 403.19 0.248 403.303281 gic co2 71906 2018-08-29 11:00:00 gic 20.0 402.07 0.366 403.370067 gic co2 71907 rows \u00d7 8 columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name lat lon alt code bik Bialystok 53.231998 23.027000 183.0 bik bir Birkenes Observatory 58.388600 8.251900 219.0 bir bis Biscarrosse 44.378100 -1.231100 73.0 bis brm Beromunster 47.189600 8.175500 797.0 brm bsd Bilsdale 54.359000 -1.150000 380.0 bsd ces Cabauw 51.971000 4.927000 -1.0 ces cmn Monte Cimone 44.166668 10.683333 2165.0 cmn crp Carnsore Point 52.180000 -6.370000 9.0 crp dec Delta de l'Ebre 40.743900 0.786700 1.0 dec eec El Estrecho 36.058600 -5.664000 20.0 eec ers Ersa 42.969200 9.380100 533.0 ers fkl Finokalia 35.337800 25.669400 150.0 fkl gat Gartow 53.065700 11.442900 70.0 gat gic Sierra de Gredos 40.345700 -5.175500 1436.0 gic hei Heidelberg 49.417000 8.674000 116.0 hei hpb Hohenpeissenberg 47.801100 11.024600 934.0 hpb htm Hyltemossa 56.097600 13.418900 115.0 htm hun Hegyhatsal 46.950000 16.650000 248.0 hun ipr Ispra 45.814700 8.636000 210.0 ipr jfj Jungfraujoch 46.550000 7.987000 3570.0 jfj kas Kasprowy Wierch, High Tatra 49.232500 19.981800 1989.0 kas kre K\u0159e\u0161\u00edn u Pacova 49.572000 15.080000 534.0 kre lhw Laegern-Hochwacht 47.482200 8.397300 840.0 lhw lin Lindenberg 52.166300 14.122600 73.0 lin lmp Lampedusa 35.530000 12.520000 45.0 lmp lmu La Muela 41.594100 -1.100300 571.0 lmu lut Lutjewad 53.403600 6.352800 1.0 lut mhd Mace Head 53.326100 -9.903600 5.0 mhd mlh Malin Head 55.355000 -7.333000 22.0 mlh nor Norunda 60.086400 17.479400 46.0 nor ohp Observatoire de Haute Provence 43.931000 5.712000 650.0 ohp ope Observatoire p\u00e9renne de l'environnement 48.561900 5.503600 390.0 ope pal Pallas-Sammaltunturi, GAW Station 67.973300 24.115700 565.0 pal pdm Pic du Midi 42.937200 0.141100 2877.0 pdm prs Plateau Rosa Station 45.930000 7.700000 3480.0 prs pui Puijo 62.909600 27.654900 232.0 pui puy Puy de D\u00f4me 45.771900 2.965800 1465.0 puy rgl Ridge Hill 51.997600 -2.540000 204.0 rgl sac Saclay 48.722700 2.142000 160.0 sac smr Hyyti\u00e4l\u00e4 61.847400 24.294700 181.0 smr ssl Schauinsland, Baden-Wuerttemberg 47.920000 7.920000 1205.0 ssl svb Svartberget 64.256000 19.775000 235.0 svb tac Tacolneston 52.517700 1.138600 56.0 tac trn Trainou 47.964700 2.112500 131.0 trn uto Ut\u00f6 - Baltic sea 59.783900 21.367200 8.0 uto wao Weybourne, Norfolk 52.950200 1.121900 20.0 wao Construct the emission file LUMIA requires all the emissions to be in a netCDF4 file, covering the entire inversion period. The file can be generated from pre-processed annual, category-specific emission files. The emission file for the simulation is constructed based on keys in the emissions section of the configuration file: in our case, there is a single co2 tracer the pre-processed emission files start with the prefix \" flux_co2. \" ( emissions.co2.prefix key) there are two categories under the emissions.co2.categories section: fossil (EDGARv4.3_BP2019) biosphere (VPRM) fluxes are hourly ( emissions.co2.interval ) and in the path given by emissions.co2.path Therefore, lumia will take biosphere fluxes from the flux_co2.VPRM.%Y.nc files, and fossil emissions from the flux_co2.EDGARv4.3_BP2019.%Y.nc files. The fluxes will be located in the folder ${emissions.co2.path}/${emissions.co2.interval} (i.e. data/fluxes/nc/eurocom025x025/1h ). The emissions can be constructed using the ui.prepare_emis method. Check that the values it prints are realistic! emis = ui.prepare_emis(rcf) 2023-01-03 21:52:19.670 | INFO | lumia.formatters.xr:print_summary:287 - =============================== 2023-01-03 21:52:19.681 | INFO | lumia.formatters.xr:print_summary:288 - fossil: 2023-01-03 21:52:19.686 | INFO | lumia.formatters.xr:print_summary:290 - 2018: 2023-01-03 21:52:19.692 | INFO | lumia.formatters.xr:print_summary:293 - January: 0.15 petagC 2023-01-03 21:52:19.694 | INFO | lumia.formatters.xr:print_summary:293 - February: 0.13 petagC 2023-01-03 21:52:19.696 | INFO | lumia.formatters.xr:print_summary:293 - March: 0.14 petagC 2023-01-03 21:52:19.698 | INFO | lumia.formatters.xr:print_summary:293 - April: 0.12 petagC 2023-01-03 21:52:19.699 | INFO | lumia.formatters.xr:print_summary:293 - May: 0.12 petagC 2023-01-03 21:52:19.700 | INFO | lumia.formatters.xr:print_summary:293 - June: 0.10 petagC 2023-01-03 21:52:19.701 | INFO | lumia.formatters.xr:print_summary:293 - July: 0.10 petagC 2023-01-03 21:52:19.704 | INFO | lumia.formatters.xr:print_summary:293 - August: 0.11 petagC 2023-01-03 21:52:19.705 | INFO | lumia.formatters.xr:print_summary:293 - September: 0.11 petagC 2023-01-03 21:52:19.706 | INFO | lumia.formatters.xr:print_summary:293 - October: 0.13 petagC 2023-01-03 21:52:19.708 | INFO | lumia.formatters.xr:print_summary:293 - November: 0.13 petagC 2023-01-03 21:52:19.710 | INFO | lumia.formatters.xr:print_summary:293 - December: 0.14 petagC 2023-01-03 21:52:19.711 | INFO | lumia.formatters.xr:print_summary:294 - -------------------------- 2023-01-03 21:52:19.711 | INFO | lumia.formatters.xr:print_summary:295 - Total : 1.48 petagC 2023-01-03 21:52:20.535 | INFO | lumia.formatters.xr:print_summary:287 - =============================== 2023-01-03 21:52:20.537 | INFO | lumia.formatters.xr:print_summary:288 - biosphere: 2023-01-03 21:52:20.539 | INFO | lumia.formatters.xr:print_summary:290 - 2018: 2023-01-03 21:52:20.541 | INFO | lumia.formatters.xr:print_summary:293 - January: 0.12 petagC 2023-01-03 21:52:20.542 | INFO | lumia.formatters.xr:print_summary:293 - February: 0.09 petagC 2023-01-03 21:52:20.543 | INFO | lumia.formatters.xr:print_summary:293 - March: 0.07 petagC 2023-01-03 21:52:20.545 | INFO | lumia.formatters.xr:print_summary:293 - April: -0.18 petagC 2023-01-03 21:52:20.547 | INFO | lumia.formatters.xr:print_summary:293 - May: -0.61 petagC 2023-01-03 21:52:20.548 | INFO | lumia.formatters.xr:print_summary:293 - June: -0.60 petagC 2023-01-03 21:52:20.550 | INFO | lumia.formatters.xr:print_summary:293 - July: -0.45 petagC 2023-01-03 21:52:20.551 | INFO | lumia.formatters.xr:print_summary:293 - August: -0.23 petagC 2023-01-03 21:52:20.553 | INFO | lumia.formatters.xr:print_summary:293 - September: -0.03 petagC 2023-01-03 21:52:20.554 | INFO | lumia.formatters.xr:print_summary:293 - October: 0.11 petagC 2023-01-03 21:52:20.556 | INFO | lumia.formatters.xr:print_summary:293 - November: 0.12 petagC 2023-01-03 21:52:20.557 | INFO | lumia.formatters.xr:print_summary:293 - December: 0.11 petagC 2023-01-03 21:52:20.558 | INFO | lumia.formatters.xr:print_summary:294 - -------------------------- 2023-01-03 21:52:20.559 | INFO | lumia.formatters.xr:print_summary:295 - Total : -1.49 petagC Setup the transport model The lumia.transport class handles the communication between lumia and the (pseudo-) transport model. The \"formatter\" is a module containing a WriteStruct and a ReadStruct functions, whose task is to write/read data drivers data for the transport model (and output data of its adjoint). model = lumia.transport(rcf, obs=obs, formatter=xr) Setup the observation uncertainties. In this example, we use the dyn approach. The obs uncertainty (which accounts for model error) is estimated based on the quality of the fit to the short-term observed variability. This works the following way: 1. A forward model run is performed, with prior emissions 2. long-term variability (> 7 days) is removed from both the modelled and the observed concentrations (this is done by subtracting their 7-days moving average) 3. the obs uncertainty is the standard deviation of the fit of the modelled detrended concentrations to the observed ones. The rationale is that, since the inversion only optimize emissions at a weekly interval (in this example), shorter variability cannot be improved and is therefore necessarily a feature of the model uncertainty. Note that this technique requires performing a forward model run. Other approaches are implemented but haven't necessarily been updated to the yaml config file, so adjustments in the code might be needed (in the obsdb/InversionDb.py file) model = ui.setup_uncertainties(model, emis) The plots below illustrate the calculation and comparison of short-term variability at one example site: dbs = model.db['bik'] f, ax = subplots(2, 1, figsize=(16, 8)) ax[0].plot(dbs.time, dbs.obs, 'k.', label='obs', ms=1) ax[0].plot(dbs.time, dbs.obs_detrended, 'k-', label='obs detrended') ax[0].plot(dbs.time, dbs.mix_apri, 'r.', label='apri', ms=1) ax[0].plot(dbs.time, dbs.mod_detrended, 'r-', label='apri detrended') ax[0].grid() ax[0].legend() ax[0].set_title('concentrations at Byalistok') ax[1].plot(dbs.time, dbs.resid_obs, 'k-', label='short term variability obs') ax[1].plot(dbs.time, dbs.resid_mod, 'r-', label='short term variability model') ax[1].grid() ax[1].legend() sig = (dbs.resid_mod - dbs.resid_obs).std() ax[1].set_title(f'Short-term variability of the concentration at Byalistok (sigma = {sig:.2f} ppm)') Text(0.5, 1.0, 'Short-term variability of the concentration at Byalistok (sigma = 3.99 ppm)') Definition of the state vector The inversion adjusts 2500 pixels or cluster of pixels every week (or whatever values set by the optimize.emissions.co2.*.npoints and optimize.emissions.co2.*.optimization_interval keys). The grouping of pixels in clusters is based on the sensitivity of the observation network to the emissions: pixels not well monitored by the observation network will tend to be grouped together, while pixels directly upwind of the measurement stations will be optimized independently. This clustering is calculated dynamically, based on an initial adjoint run: sensi = model.calcSensitivityMap(emis) Below the Interface is the module that handles the transitions between optimization space (state vector, 2500 x n_weeks points x n_tracers x n_cat) and the model space (gridded fluxes). control = Interface(model.rcf, model_data=emis, sensi_map=sensi) The plots below illustrate the calculated sensitivity of the observation network to the surface fluxes (left panel), and the resulting clustering of emissions (right panel, the colors are random). Note that there are only footprints for two sites in the example data used to generate this notebook. In a more realistic case, the maps would look somewhat different ... f, ax = subplots(1, 2, figsize=(16, 8), subplot_kw=dict(projection=cartopy.crs.PlateCarree(), extent=rcf['run']['grid'].extent)) ax[0].coastlines() ax[0].imshow(log(sensi['co2']), extent=rcf['run']['grid'].extent, origin='lower') ax[0].set_title(\"sensitivity of the network to the fluxes (log scale)\") smap = control.model_data.co2.spatial_mapping['biosphere'].values.copy() for ii in range(smap.shape[1]): smap[smap[:, ii] != 0, ii] = randint(0, 1000) ax[1].imshow(smap.sum(1).reshape((160, 200)), origin='lower', extent=rcf['run']['grid'].extent) ax[1].coastlines() ax[1].set_title('Optimized clusters') Text(0.5, 1.0, 'Optimized clusters') Run the inversion The prior error-covariance matrix will be calculated when initializing the optimizer (first line below). opt = lumia.optimizer.Optimizer(model.rcf, model, control) opt.Var4D()","title":"Step-by-step inversion tutorial"},{"location":"tutorial/#step-by-step-inversion-tutorial","text":"This tutorial shows how to run a simple CO2 inversion, using example data (download from the ICOS Carbon Portal ). This assumes that: LUMIA has been installed The example observation file (obs_example.tgz) and configuration file (inversion.yaml) are present in the current folder Footprint files are present on disk, in the ./footprints folder Pre-processed emission files are present in the ./data/fluxes/eurocom025x025/1h folder If the data is in a different place, edit the config file (inversion.yaml). The step-by-step procedure described below is the equivalent of just running lumia optim --rc inversion.yaml .","title":"Step-by-step inversion tutorial"},{"location":"tutorial/#import-modules","text":"# Modules required by the inversion from lumia import ui # user-interface (higher-level methods) from lumia.config import RcFile # Config files from lumia.formatters import xr # emission files from lumia.interfaces.multitracer import Interface # Interface betwen the model state (gridded fluxes) and the optimization (state vector) import lumia # Modules required for display purpose in this notebook: from IPython.display import display from matplotlib.pyplot import subplots import cartopy from numpy import log from numpy.random import randint","title":"Import modules"},{"location":"tutorial/#load-the-configuration-file","text":"Detailed description of the config file can be found in the Settings section. rcf = RcFile('inversion.yaml')","title":"Load the configuration file"},{"location":"tutorial/#load-the-observations-file","text":"The observation file contains two pandas dataframes: the observations DataFrame contains the observation themselves. Mandatory columns are time, site, height, obs, err_obs, mix_background, code and tracer: the sites DataFrame contains the information that is common to all obs of one site (lat, lon, alt, site name and site code). There not really any mandatory column ... In the observations table, the site column contains indices of the sites table (leftermost column in the output below), while the code column contains the site codes (they are similar in this instance, but site could be anything). height is the sampling height (above ground), while alt is the ground altitude (above sea level) at the sites. err_obs is the measurement uncertainty. Use a value below 0 if unavailable. obs = ui.load_observations(rcf) display(obs.observations) display(obs.sites) 2023-01-04 16:55:51.852 | INFO | lumia.obsdb:load_tar:169 - 71907 observation read from obs_example.tgz .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } time site height obs err_obs mix_background code tracer 0 2018-01-03 23:00:00 ssl 12.0 411.95 -9.990 410.626136 ssl co2 1 2018-01-04 00:00:00 ssl 12.0 412.03 -9.990 410.596810 ssl co2 2 2018-01-04 01:00:00 ssl 12.0 412.04 -9.990 410.560833 ssl co2 3 2018-01-04 02:00:00 ssl 12.0 411.59 -9.990 410.515688 ssl co2 4 2018-01-04 03:00:00 ssl 12.0 411.73 -9.990 410.463696 ssl co2 ... ... ... ... ... ... ... ... ... 71902 2018-08-28 12:00:00 gic 20.0 402.39 0.222 402.745598 gic co2 71903 2018-08-28 13:00:00 gic 20.0 402.42 0.285 402.983801 gic co2 71904 2018-08-28 14:00:00 gic 20.0 402.61 0.438 403.182289 gic co2 71905 2018-08-28 15:00:00 gic 20.0 403.19 0.248 403.303281 gic co2 71906 2018-08-29 11:00:00 gic 20.0 402.07 0.366 403.370067 gic co2 71907 rows \u00d7 8 columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name lat lon alt code bik Bialystok 53.231998 23.027000 183.0 bik bir Birkenes Observatory 58.388600 8.251900 219.0 bir bis Biscarrosse 44.378100 -1.231100 73.0 bis brm Beromunster 47.189600 8.175500 797.0 brm bsd Bilsdale 54.359000 -1.150000 380.0 bsd ces Cabauw 51.971000 4.927000 -1.0 ces cmn Monte Cimone 44.166668 10.683333 2165.0 cmn crp Carnsore Point 52.180000 -6.370000 9.0 crp dec Delta de l'Ebre 40.743900 0.786700 1.0 dec eec El Estrecho 36.058600 -5.664000 20.0 eec ers Ersa 42.969200 9.380100 533.0 ers fkl Finokalia 35.337800 25.669400 150.0 fkl gat Gartow 53.065700 11.442900 70.0 gat gic Sierra de Gredos 40.345700 -5.175500 1436.0 gic hei Heidelberg 49.417000 8.674000 116.0 hei hpb Hohenpeissenberg 47.801100 11.024600 934.0 hpb htm Hyltemossa 56.097600 13.418900 115.0 htm hun Hegyhatsal 46.950000 16.650000 248.0 hun ipr Ispra 45.814700 8.636000 210.0 ipr jfj Jungfraujoch 46.550000 7.987000 3570.0 jfj kas Kasprowy Wierch, High Tatra 49.232500 19.981800 1989.0 kas kre K\u0159e\u0161\u00edn u Pacova 49.572000 15.080000 534.0 kre lhw Laegern-Hochwacht 47.482200 8.397300 840.0 lhw lin Lindenberg 52.166300 14.122600 73.0 lin lmp Lampedusa 35.530000 12.520000 45.0 lmp lmu La Muela 41.594100 -1.100300 571.0 lmu lut Lutjewad 53.403600 6.352800 1.0 lut mhd Mace Head 53.326100 -9.903600 5.0 mhd mlh Malin Head 55.355000 -7.333000 22.0 mlh nor Norunda 60.086400 17.479400 46.0 nor ohp Observatoire de Haute Provence 43.931000 5.712000 650.0 ohp ope Observatoire p\u00e9renne de l'environnement 48.561900 5.503600 390.0 ope pal Pallas-Sammaltunturi, GAW Station 67.973300 24.115700 565.0 pal pdm Pic du Midi 42.937200 0.141100 2877.0 pdm prs Plateau Rosa Station 45.930000 7.700000 3480.0 prs pui Puijo 62.909600 27.654900 232.0 pui puy Puy de D\u00f4me 45.771900 2.965800 1465.0 puy rgl Ridge Hill 51.997600 -2.540000 204.0 rgl sac Saclay 48.722700 2.142000 160.0 sac smr Hyyti\u00e4l\u00e4 61.847400 24.294700 181.0 smr ssl Schauinsland, Baden-Wuerttemberg 47.920000 7.920000 1205.0 ssl svb Svartberget 64.256000 19.775000 235.0 svb tac Tacolneston 52.517700 1.138600 56.0 tac trn Trainou 47.964700 2.112500 131.0 trn uto Ut\u00f6 - Baltic sea 59.783900 21.367200 8.0 uto wao Weybourne, Norfolk 52.950200 1.121900 20.0 wao","title":"Load the observations file."},{"location":"tutorial/#construct-the-emission-file","text":"LUMIA requires all the emissions to be in a netCDF4 file, covering the entire inversion period. The file can be generated from pre-processed annual, category-specific emission files. The emission file for the simulation is constructed based on keys in the emissions section of the configuration file: in our case, there is a single co2 tracer the pre-processed emission files start with the prefix \" flux_co2. \" ( emissions.co2.prefix key) there are two categories under the emissions.co2.categories section: fossil (EDGARv4.3_BP2019) biosphere (VPRM) fluxes are hourly ( emissions.co2.interval ) and in the path given by emissions.co2.path Therefore, lumia will take biosphere fluxes from the flux_co2.VPRM.%Y.nc files, and fossil emissions from the flux_co2.EDGARv4.3_BP2019.%Y.nc files. The fluxes will be located in the folder ${emissions.co2.path}/${emissions.co2.interval} (i.e. data/fluxes/nc/eurocom025x025/1h ). The emissions can be constructed using the ui.prepare_emis method. Check that the values it prints are realistic! emis = ui.prepare_emis(rcf) 2023-01-03 21:52:19.670 | INFO | lumia.formatters.xr:print_summary:287 - =============================== 2023-01-03 21:52:19.681 | INFO | lumia.formatters.xr:print_summary:288 - fossil: 2023-01-03 21:52:19.686 | INFO | lumia.formatters.xr:print_summary:290 - 2018: 2023-01-03 21:52:19.692 | INFO | lumia.formatters.xr:print_summary:293 - January: 0.15 petagC 2023-01-03 21:52:19.694 | INFO | lumia.formatters.xr:print_summary:293 - February: 0.13 petagC 2023-01-03 21:52:19.696 | INFO | lumia.formatters.xr:print_summary:293 - March: 0.14 petagC 2023-01-03 21:52:19.698 | INFO | lumia.formatters.xr:print_summary:293 - April: 0.12 petagC 2023-01-03 21:52:19.699 | INFO | lumia.formatters.xr:print_summary:293 - May: 0.12 petagC 2023-01-03 21:52:19.700 | INFO | lumia.formatters.xr:print_summary:293 - June: 0.10 petagC 2023-01-03 21:52:19.701 | INFO | lumia.formatters.xr:print_summary:293 - July: 0.10 petagC 2023-01-03 21:52:19.704 | INFO | lumia.formatters.xr:print_summary:293 - August: 0.11 petagC 2023-01-03 21:52:19.705 | INFO | lumia.formatters.xr:print_summary:293 - September: 0.11 petagC 2023-01-03 21:52:19.706 | INFO | lumia.formatters.xr:print_summary:293 - October: 0.13 petagC 2023-01-03 21:52:19.708 | INFO | lumia.formatters.xr:print_summary:293 - November: 0.13 petagC 2023-01-03 21:52:19.710 | INFO | lumia.formatters.xr:print_summary:293 - December: 0.14 petagC 2023-01-03 21:52:19.711 | INFO | lumia.formatters.xr:print_summary:294 - -------------------------- 2023-01-03 21:52:19.711 | INFO | lumia.formatters.xr:print_summary:295 - Total : 1.48 petagC 2023-01-03 21:52:20.535 | INFO | lumia.formatters.xr:print_summary:287 - =============================== 2023-01-03 21:52:20.537 | INFO | lumia.formatters.xr:print_summary:288 - biosphere: 2023-01-03 21:52:20.539 | INFO | lumia.formatters.xr:print_summary:290 - 2018: 2023-01-03 21:52:20.541 | INFO | lumia.formatters.xr:print_summary:293 - January: 0.12 petagC 2023-01-03 21:52:20.542 | INFO | lumia.formatters.xr:print_summary:293 - February: 0.09 petagC 2023-01-03 21:52:20.543 | INFO | lumia.formatters.xr:print_summary:293 - March: 0.07 petagC 2023-01-03 21:52:20.545 | INFO | lumia.formatters.xr:print_summary:293 - April: -0.18 petagC 2023-01-03 21:52:20.547 | INFO | lumia.formatters.xr:print_summary:293 - May: -0.61 petagC 2023-01-03 21:52:20.548 | INFO | lumia.formatters.xr:print_summary:293 - June: -0.60 petagC 2023-01-03 21:52:20.550 | INFO | lumia.formatters.xr:print_summary:293 - July: -0.45 petagC 2023-01-03 21:52:20.551 | INFO | lumia.formatters.xr:print_summary:293 - August: -0.23 petagC 2023-01-03 21:52:20.553 | INFO | lumia.formatters.xr:print_summary:293 - September: -0.03 petagC 2023-01-03 21:52:20.554 | INFO | lumia.formatters.xr:print_summary:293 - October: 0.11 petagC 2023-01-03 21:52:20.556 | INFO | lumia.formatters.xr:print_summary:293 - November: 0.12 petagC 2023-01-03 21:52:20.557 | INFO | lumia.formatters.xr:print_summary:293 - December: 0.11 petagC 2023-01-03 21:52:20.558 | INFO | lumia.formatters.xr:print_summary:294 - -------------------------- 2023-01-03 21:52:20.559 | INFO | lumia.formatters.xr:print_summary:295 - Total : -1.49 petagC","title":"Construct the emission file"},{"location":"tutorial/#setup-the-transport-model","text":"The lumia.transport class handles the communication between lumia and the (pseudo-) transport model. The \"formatter\" is a module containing a WriteStruct and a ReadStruct functions, whose task is to write/read data drivers data for the transport model (and output data of its adjoint). model = lumia.transport(rcf, obs=obs, formatter=xr)","title":"Setup the transport model"},{"location":"tutorial/#setup-the-observation-uncertainties","text":"In this example, we use the dyn approach. The obs uncertainty (which accounts for model error) is estimated based on the quality of the fit to the short-term observed variability. This works the following way: 1. A forward model run is performed, with prior emissions 2. long-term variability (> 7 days) is removed from both the modelled and the observed concentrations (this is done by subtracting their 7-days moving average) 3. the obs uncertainty is the standard deviation of the fit of the modelled detrended concentrations to the observed ones. The rationale is that, since the inversion only optimize emissions at a weekly interval (in this example), shorter variability cannot be improved and is therefore necessarily a feature of the model uncertainty. Note that this technique requires performing a forward model run. Other approaches are implemented but haven't necessarily been updated to the yaml config file, so adjustments in the code might be needed (in the obsdb/InversionDb.py file) model = ui.setup_uncertainties(model, emis) The plots below illustrate the calculation and comparison of short-term variability at one example site: dbs = model.db['bik'] f, ax = subplots(2, 1, figsize=(16, 8)) ax[0].plot(dbs.time, dbs.obs, 'k.', label='obs', ms=1) ax[0].plot(dbs.time, dbs.obs_detrended, 'k-', label='obs detrended') ax[0].plot(dbs.time, dbs.mix_apri, 'r.', label='apri', ms=1) ax[0].plot(dbs.time, dbs.mod_detrended, 'r-', label='apri detrended') ax[0].grid() ax[0].legend() ax[0].set_title('concentrations at Byalistok') ax[1].plot(dbs.time, dbs.resid_obs, 'k-', label='short term variability obs') ax[1].plot(dbs.time, dbs.resid_mod, 'r-', label='short term variability model') ax[1].grid() ax[1].legend() sig = (dbs.resid_mod - dbs.resid_obs).std() ax[1].set_title(f'Short-term variability of the concentration at Byalistok (sigma = {sig:.2f} ppm)') Text(0.5, 1.0, 'Short-term variability of the concentration at Byalistok (sigma = 3.99 ppm)')","title":"Setup the observation uncertainties."},{"location":"tutorial/#definition-of-the-state-vector","text":"The inversion adjusts 2500 pixels or cluster of pixels every week (or whatever values set by the optimize.emissions.co2.*.npoints and optimize.emissions.co2.*.optimization_interval keys). The grouping of pixels in clusters is based on the sensitivity of the observation network to the emissions: pixels not well monitored by the observation network will tend to be grouped together, while pixels directly upwind of the measurement stations will be optimized independently. This clustering is calculated dynamically, based on an initial adjoint run: sensi = model.calcSensitivityMap(emis) Below the Interface is the module that handles the transitions between optimization space (state vector, 2500 x n_weeks points x n_tracers x n_cat) and the model space (gridded fluxes). control = Interface(model.rcf, model_data=emis, sensi_map=sensi) The plots below illustrate the calculated sensitivity of the observation network to the surface fluxes (left panel), and the resulting clustering of emissions (right panel, the colors are random). Note that there are only footprints for two sites in the example data used to generate this notebook. In a more realistic case, the maps would look somewhat different ... f, ax = subplots(1, 2, figsize=(16, 8), subplot_kw=dict(projection=cartopy.crs.PlateCarree(), extent=rcf['run']['grid'].extent)) ax[0].coastlines() ax[0].imshow(log(sensi['co2']), extent=rcf['run']['grid'].extent, origin='lower') ax[0].set_title(\"sensitivity of the network to the fluxes (log scale)\") smap = control.model_data.co2.spatial_mapping['biosphere'].values.copy() for ii in range(smap.shape[1]): smap[smap[:, ii] != 0, ii] = randint(0, 1000) ax[1].imshow(smap.sum(1).reshape((160, 200)), origin='lower', extent=rcf['run']['grid'].extent) ax[1].coastlines() ax[1].set_title('Optimized clusters') Text(0.5, 1.0, 'Optimized clusters')","title":"Definition of the state vector"},{"location":"tutorial/#run-the-inversion","text":"The prior error-covariance matrix will be calculated when initializing the optimizer (first line below). opt = lumia.optimizer.Optimizer(model.rcf, model, control) opt.Var4D()","title":"Run the inversion"}]}