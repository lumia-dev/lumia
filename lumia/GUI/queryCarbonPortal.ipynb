{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "import os\n", "import re\n", "import datetime\n", "from loguru import logger\n", "import numpy as np\n", "from pandas import DataFrame  #, concat\n", "#Import ICOS tools:\n", "# from icoscp.sparql import sparqls, runsparql\n", "from icoscp.sparql.runsparql import RunSparql\n", "from icoscp.cpb.dobj import Dobj\n", "from icoscp.collection import collection\n", "from icoscp.cpb import metadata"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bDEBUG =False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["GLOBALS<br>\n", "<br>\n", "The pre-processed data used by Lumia (as a-priori) is described e.g. here:<br>\n", "https://meta.icos-cp.eu/objects/sNkFBomuWN94yAqEXXSYAW54<br>\n", "<br>\n", "Observational data available on the ICOS data portal you can discover via the online tool at<br>\n", "old: https://data.icos-cp.eu/portal/#%7B%22filterCategories%22%3A%7B%22theme%22%3A%5B%22atmosphere%22%5D%2C%22level%22%3A%5B2%5D%2C%22valType%22%3A%5B%22co2MixingRatio%22%5D%2C%22stationclass%22%3A%5B%22ICOS%22%5D%2C%22project%22%3A%5B%22icos%22%5D%2C%22type%22%3A%5B%22atcCo2L2DataObject%22%5D%2C%22variable%22%3A%5B%22http%3A%2F%2Fmeta.icos-cp.eu%2Fresources%2Fcpmeta%2Fco2atcMoleFrac%22%5D%7D%2C%22filterTemporal%22%3A%7B%22df%22%3A%222017-12-31%22%2C%22dt%22%3A%222018-01-31%22%7D%2C%22tabs%22%3A%7B%22resultTab%22%3A1%7D%7D"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can try out sparql queries and export the ones you are happy with, with the button<br>\n", "\"Open the SPARQL query basic search result\" page - the icon looks like an arrow pointing up and then to the right out of a stylized box."]}, {"cell_type": "markdown", "metadata": {}, "source": ["However, it is usually better to keep the url rather than the exported resulting script, as the url is better suited to discover new data, whenever new data sets are added. <br>\n", "Also, you may want to tidy up the resulting query in a similar fashion to the example provided here, that Oleg prettied up for me:<br>\n", "<br>\n", "https://data.icos-cp.eu/portal/#%7B%22filterCategories%22%3A%7B%22valType%22%3A%5B%22co2MixingRatioMolMol%22%2C%22co2MixingRatio%22%5D%2C%22theme%22%3A%5B%22atmosphere%22%5D%7D%7D<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/><br>\n", "prefix prov: <http://www.w3.org/ns/prov#><br>\n", "prefix xsd: <http://www.w3.org/2001/XMLSchema#><br>\n", "select ?dobj ?samplingHeight ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd<br>\n", "where {<br>\n", "    VALUES ?spec {<br>\n", "        <http://meta.icos-cp.eu/resources/cpmeta/atcCo2L2DataObject><br>\n", "        <http://meta.icos-cp.eu/resources/cpmeta/atcCo2Product><br>\n", "        <http://meta.icos-cp.eu/resources/cpmeta/ObspackTimeSerieResult><br>\n", "    }<br>\n", "    ?dobj cpmeta:hasObjectSpec ?spec .<br>\n", "    ?dobj cpmeta:hasSizeInBytes ?size .<br>\n", "    ?dobj cpmeta:hasName ?fileName .<br>\n", "    ?dobj cpmeta:wasSubmittedBy/prov:endedAtTime ?submTime .<br>\n", "    ?dobj cpmeta:wasAcquiredBy / prov:startedAtTime ?timeStart .<br>\n", "    ?dobj cpmeta:wasAcquiredBy / prov:endedAtTime ?timeEnd .<br>\n", "    OPTIONAL{?dobj cpmeta:wasAcquiredBy / cpmeta:hasSamplingHeight ?samplingHeight }<br>\n", "    FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}<br>\n", "FILTER( !(?timeStart > '2018-12-30T23:00:00.000Z'^^xsd:dateTime || ?timeEnd < '2017-12-31T23:00:00.000Z'^^xsd:dateTime) )<br>\n", "}<br>\n", "order by desc(?submTime)<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["*****************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getTimeForSparqlQuery(pdTime,  startOrEndTime=None,  timeStep=None):\n", "    '''\n", "    Function getEndTimeForSparqlQuery\n", "    \n", "    @param pdTime pandas.timeStamp  date/time from/until when observations will be queried\n", "    @returns a DateTimeStamp string of format 'YYYY-01-01T00:00:01.000Z'\n", "    @timeStep is the run.timestep defined in the user's rc configuration file'\n", "  '''\n", "    if((pdTime is None) or (startOrEndTime is None)):\n", "        logger.error('Start date of observation interval or startOrEndTime operator not provided.')\n", "        sys.exit(-1)\n", "    if('start' in startOrEndTime):\n", "        operation='subtractTime'\n", "    else:\n", "        operation='addTime'\n", "    delta=int(60)\n", "    if((timeStep is not None) and (len(timeStep)>1)):\n", "        if('h' in timeStep):\n", "            delta=int(timeStep[:-1])*60\n", "        elif('m' in timeStep):\n", "            delta=int(timeStep[:-1])\n", "        elif('d' in timeStep):\n", "            delta=int(timeStep[:-1])*60*24\n", "    if('add' in operation):\n", "        endTime=pdTime + datetime.timedelta(minutes=delta)            \n", "    else:\n", "        endTime=pdTime - datetime.timedelta(minutes=delta)            \n", "    sSparqlTime=str(endTime)\n", "    sSparqlTime=sSparqlTime[:10]+'T'+sSparqlTime[11:19]+'.000Z'\n", "    return sSparqlTime"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***********************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getCo2DryMolFractionObjectsFromSparql(pdTimeStart: datetime=None, pdTimeEnd: datetime=None, timeStep=None,iVerbosityLv=1):\n", "    '''\n", "    Function \n", "    @param pdTimeStart :  from when on we want to get the observations\n", "    @type datetime\n", "    @param pdTimeEnd : until when on we want to get the observations\n", "    @type datetime\n", "    @param iVerbosityLv : defines how much detail of program progress is printed to stdout (defaults to 1)\n", "    @type integer between 0 and 3 (optional)\n", "    @return :  dobj (the data object returned by the SPARQL query)\n", "    @rtype dobj (a structured dictionary of strings)\n", "    '''\n", "    sTimeStart=getTimeForSparqlQuery(pdTimeStart,  startOrEndTime='startTime',  timeStep=timeStep)\n", "    sTimeEnd=getTimeForSparqlQuery(pdTimeEnd,  startOrEndTime='endTime',  timeStep=timeStep)\n", "    query = '''\n", "    prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n", "    prefix prov: <http://www.w3.org/ns/prov#>\n", "    prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n", "    select ?dobj ?samplingHeight ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n", "    where {\n", "        VALUES ?spec {\n", "            <http://meta.icos-cp.eu/resources/cpmeta/atcCo2L2DataObject>\n", "            <http://meta.icos-cp.eu/resources/cpmeta/atcCo2Product>\n", "            <http://meta.icos-cp.eu/resources/cpmeta/ObspackTimeSerieResult>\n", "        }\n", "        ?dobj cpmeta:hasObjectSpec ?spec .\n", "        ?dobj cpmeta:hasSizeInBytes ?size .\n", "        ?dobj cpmeta:hasName ?fileName .\n", "        ?dobj cpmeta:wasSubmittedBy/prov:endedAtTime ?submTime .\n", "        ?dobj cpmeta:wasAcquiredBy / prov:startedAtTime ?timeStart .\n", "        ?dobj cpmeta:wasAcquiredBy / prov:endedAtTime ?timeEnd .\n", "        OPTIONAL{?dobj cpmeta:wasAcquiredBy / cpmeta:hasSamplingHeight ?samplingHeight }\n", "        FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n", "    FILTER( !(?timeStart > \\''''+sTimeEnd+'''\\'^^xsd:dateTime || ?timeEnd < \\''''+sTimeStart+'''\\'^^xsd:dateTime) ) \n", "    }\n", "    order by desc(?submTime)\n", "    '''\n", "    # example: sFileName='VPRM_ECMWF_NEE_2020_CP.nc'\n", "    logger.debug(f'SPARQL query= {query}')    \n", "    dobj = RunSparql(query,output_format='nc').run()\n", "    # logger.debug(f'dobj= {dobj}')\n", "    return(dobj)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***********************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def getDobjFromSparql(tracer='CO2', pdTimeStart: datetime=None, pdTimeEnd: datetime=None, timeStep=None,  sDataType=None,  iVerbosityLv=1):\n", "    '''\n", "    Function  -- for historical and possible future use with ch4. \n", "    Presently Lumia only uses the function getCo2DryMolFractionObjectsFromSparql()\n", "    @param tracer :    the name of the tracer like co2, ch4, etc.\n", "    @type string \n", "    @param pdTimeStart :  from when on we want to get the observations\n", "    @type datetime\n", "    @param pdTimeEnd : until when on we want to get the observations\n", "    @type datetime\n", "    @param sDataType optional name of the data type. Default is 'atcCo2L2DataObject'  #'ICOS ATC CO2 Release'\n", "    @type string\n", "    @param iVerbosityLv : defines how much detail of program progress is printed to stdout (defaults to 1)\n", "    @type integer between 0 and 3 (optional)\n", "    @return :  dobj (the data object returned by the SPARQL query)\n", "    @rtype dobj (a structured dictionary of strings)\n", "    '''\n", "    # sTimeStart=getStartTimeForSparqlQuery(pdTimeStart, timeStep)\n", "    sTimeStart=getTimeForSparqlQuery(pdTimeStart,  startOrEndTime='startTime',  timeStep=timeStep)\n", "    # sTimeEnd=getEndTimeForSparqlQuery(pdTimeEnd, timeStep)\n", "    sTimeEnd=getTimeForSparqlQuery(pdTimeEnd,  startOrEndTime='endTime',  timeStep=timeStep)\n", "    LcTracer=tracer.lower()\n", "    if(sDataType is None):\n", "        sDataType='atcCo2L2DataObject'  #'ICOS ATC CO2 Release'\n\n", "    #=findDobjFromPartialNameAndDate(sKeyword, timeStart, timeEnd, iRequestedYear)\n", "    query = '''\n", "    prefix cpmeta: <http://meta.icos-cp.eu/ontologies/cpmeta/>\n", "    prefix prov: <http://www.w3.org/ns/prov#>\n", "    prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n", "    select ?dobj ?hasNextVersion ?spec ?fileName ?size ?submTime ?timeStart ?timeEnd\n", "    where {\n", "        VALUES ?spec {<http://meta.icos-cp.eu/resources/cpmeta/'''+sDataType+'''>}\n", "        ?dobj cpmeta:hasObjectSpec ?spec .\n", "        BIND(EXISTS{[] cpmeta:isNextVersionOf ?dobj} AS ?hasNextVersion)\n", "        VALUES ?station {<http://meta.icos-cp.eu/resources/stations/AS_PAL> <http://meta.icos-cp.eu/resources/stations/AS_TRN> <http://meta.icos-cp.eu/resources/stations/AS_GAT> <http://meta.icos-cp.eu/resources/stations/AS_HPB> <http://meta.icos-cp.eu/resources/stations/AS_IPR> <http://meta.icos-cp.eu/resources/stations/AS_OPE> <http://meta.icos-cp.eu/resources/stations/AS_KIT> <http://meta.icos-cp.eu/resources/stations/AS_SMR> <http://meta.icos-cp.eu/resources/stations/AS_SAC> <http://meta.icos-cp.eu/resources/stations/AS_ZEP> <http://meta.icos-cp.eu/resources/stations/AS_TOH> <http://meta.icos-cp.eu/resources/stations/AS_KRE> <http://meta.icos-cp.eu/resources/stations/AS_SVB> <http://meta.icos-cp.eu/resources/stations/AS_HTM> <http://meta.icos-cp.eu/resources/stations/AS_JFJ> <http://meta.icos-cp.eu/resources/stations/AS_PUY> <http://meta.icos-cp.eu/resources/stations/AS_NOR> <http://meta.icos-cp.eu/resources/stations/AS_LIN>}\n", "                ?dobj cpmeta:wasAcquiredBy/prov:wasAssociatedWith ?station .\n", "        ?dobj cpmeta:hasSizeInBytes ?size .\n", "    ?dobj cpmeta:hasName ?fileName .\n", "    ?dobj cpmeta:wasSubmittedBy/prov:endedAtTime ?submTime .\n", "    ?dobj cpmeta:hasStartTime | (cpmeta:wasAcquiredBy / prov:startedAtTime) ?timeStart .\n", "    ?dobj cpmeta:hasEndTime | (cpmeta:wasAcquiredBy / prov:endedAtTime) ?timeEnd .\n", "        FILTER NOT EXISTS {[] cpmeta:isNextVersionOf ?dobj}\n", "    FILTER( !(?timeStart > \\''''+sTimeEnd+'''\\'^^xsd:dateTime || ?timeEnd < \\''''+sTimeStart+'''\\'^^xsd:dateTime) ) \n", "        {\n", "            {FILTER NOT EXISTS {?dobj cpmeta:hasVariableName ?varName}}\n", "            UNION\n", "            {\n", "                ?dobj cpmeta:hasVariableName ?varName\n", "                FILTER (?varName = \"'''+LcTracer+'''\")\n", "            }\n", "        }\n", "    }\n", "    order by desc(?submTime)\n", "    offset 0 limit 20\n", "    '''\n", "    # example: sFileName='VPRM_ECMWF_NEE_2020_CP.nc'\n", "    logger.debug(f'SPARQL query= {query}')    \n", "    dobj = RunSparql(query,output_format='nc').run()\n", "    # logger.debug(f'dobj= {dobj}')\n", "    return(dobj)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***********************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def extractFnamesFromDobj(dobj, iVerbosityLv=1):\n", "    '''\n", "    Function \n", "    @param dobj the object returned from the SPARQL query that contains the PIDs of all the files we want to extract\n", "    @cpDir Location where to look for the existance of the files we expect to find from the SPARQL query\n", "    @return a list of strings. Each strings contains the name of one file on the carbon portal that we later need to read.\n", "    @rtype List[strings]\n", "    \n", "    At the bottom of this python file you may find an example of a dobj for co2 observations for 20180101 - 20180201\n", "    It is from such a list of strings that we need to extract each and every PID for the individual files,\n", "    at least one per observation site.\n", "    \n", "    The relevant section looks typically as in this example:\n", "        \"dobj\" : {\n", "          \"type\" : \"uri\",\n", "          \"value\" : \"https://meta.icos-cp.eu/objects/LLz6BZr6LCt1Pt0w-U_DLxWZ\"\n", "        },\n", "    And we want to extract \"LLz6BZr6LCt1Pt0w-U_DLxWZ\"\n", "    '''\n", "    sFileNameOnCarbonPortal=None\n", "    sPID=''\n", "    fNameLst=[]\n", "    try:\n", "        if len(dobj.split('/')) > 1:\n", "            # \n", "            bGrabNextUrl=False\n", "            words=dobj.split(\"\\\"\") # split at quotation marks\n", "            cwLst=[]\n", "            for word in words:\n", "                if(re.search('[a-zA-Z]', word) is not None):\n", "                    cwLst.append(word)  # \n", "                    if('dobj' in word):\n", "                        bGrabNextUrl=True\n", "                    if((bGrabNextUrl==True) and ('https' in word)):\n", "                        sPID=word.split('/')[-1]  # Grab the last part of the url without directories\n", "                        # Grab the PID from the end of the http value string, may look like gibberish \"nBGgNpQxPYXBYiBuGGFp2VRF\"\n", "                        bGrabNextUrl=False\n", "                        #  /data/dataAppStorage/asciiAtcProductTimeSer/LLz6BZr6LCt1Pt0w-U_DLxWZ.cpb\n", "                        fNameLst.append(sPID)\n", "                        '''\n", "                        if(1==0): # Let's disable this for now. We use the icoscp library that read the file via the pid and we may easily miss a valid file location here.\n", "                            # TODO: observational data is in a different path copmpared to level3 netcdf files....\n", "                            # https://meta.icos-cp.eu/objects/LLz6BZr6LCt1Pt0w-U_DLxWZ\n", "                            # /data/dataAppStorage/asciiAtcProductTimeSer/LLz6BZr6LCt1Pt0w-U_DLxWZ.cpb\n", "                            cpDir=None # may get this from the config.yml file\n", "                            if (cpDir is None):\n", "                                cpDir=/data/dataAppStorage/asciiAtcProductTimeSer/\n", "                            sFileNameOnCarbonPortal = cpDir+sPID+'.cpb'\n", "                            try:\n", "                                # Make sure this file actually exists and is accessible on the portal\n", "                                f=open(sFileNameOnCarbonPortal, 'rb')\n", "                                f.close()\n", "                                if(iVerbosityLv>0):\n", "                                    logger.info(f\"Found ICOS co2 observations data file on the portal at {sFileNameOnCarbonPortal}\")\n", "                                fNameLst.append(sPID)\n", "                            except:\n", "                                logger.error('The file '+sFileNameOnCarbonPortal+' cannot be read or does not exist on the Carbon Portal or you are not running this script on the Carbon Portal. Please check first of all the directory you provided for observations.file.cpDir in your .yml resource file.')\n", "                                # sys.exit(-1)   /data/dataAppStorage/asciiAtcProductTimeSer/ZZb1E_dJQtRICzobwg0ib86C\n", "                        '''\n", "    except:\n", "        logger.error(\"No valid observational data found in SPARQL query dobj=\")\n", "        logger.error(f\"{dobj}\")\n", "        return(None)\n", "    return fNameLst"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***********************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def discoverObservationsOnCarbonPortal(tracer='CO2', pdTimeStart: datetime=None, pdTimeEnd: datetime=None, \n", "                                                                            timeStep=None,  ymlContents=None,  sDataType=None, sNow='',   iVerbosityLv=1):\n", "    \"\"\"\n", "    Function discoverObservationsOnCarbonPortal\n", "    \n", "    @param tracer :    the name of the tracer like co2, ch4, etc.\n", "    @type string \n", "    @param pdTimeStart :  from when on we want to get the observations\n", "    @type datetime\n", "    @param pdTimeEnd : until when on we want to get the observations\n", "    @type datetime\n", "    @param iVerbosityLv : defines how much detail of program progress is printed to stdout (defaults to 1)\n", "    @type integer between 0 and 3 (optional)\n", "    @return ???:  the actual data object? file list? mind you, columns are different....\n", "    @rtype \n", " \n", "    discoverObservationsOnCarbonPortal attempts to find matching ICOS CO2 observations in form of their individual unique-identifier (PID)\n", "    for the requested data records. The latter should refer to a list of level2 netcdf file (by name) on the ICOS data portal. \n", "    The function relies on a sparql query and tries to read the requested cpb files from the carbon portal using the icoscp package. \n", "    Returns (...-dataset) if successful; (None) if unsuccessful.\n", "    \"\"\"\n", "    if(tracer=='CO2'):\n", "        dobj=getCo2DryMolFractionObjectsFromSparql(pdTimeStart=pdTimeStart, pdTimeEnd=pdTimeEnd,  timeStep=timeStep, iVerbosityLv=iVerbosityLv)\n", "    else:        \n", "        dobj=getDobjFromSparql(tracer=tracer, pdTimeStart=pdTimeStart, pdTimeEnd=pdTimeEnd, timeStep=timeStep,  sDataType=sDataType,  iVerbosityLv=iVerbosityLv)\n", "    dobjLst=extractFnamesFromDobj(dobj, iVerbosityLv=iVerbosityLv)\n", "    #logger.debug(f\"dobjLst={dobjLst}\")\n", "    # remove any possible duplicates from the list of objects\n", "    finalDobjLst=set(dobjLst)\n", "    lf=len(finalDobjLst)\n", "    l=len(dobjLst)\n", "    n=l-lf\n", "    if(n>0):\n", "        logger.debug(f\"removed {n} duplicates\")\n", "    logger.info(f\"Found {lf} valid data objects on the carbon portal (dry mole fraction observation files for chosen tracer) Removing duplicates (if any)...\")\n", "    if(0>1): #obsolete bit of code with new SPARQL query...but let's first test the new query thoroughly before removing this snipet\n", "        cpCollections=collection.getIdList()\n", "        for collitem in cpCollections.values:\n", "            pid=collitem[0].split('/')[-1]\n", "           #logger.info(f\"Checking collection pid= {pid}\")\n", "            if(pid in finalDobjLst):\n", "                logger.info(f\"Removing collection {pid}\")\n", "                finalDobjLst.remove(pid)\n", "    # TODO: for simplicity let's reject for now the huge 1972-2023 Obspack as it is an aggregate of dataframes that I need to deal with in a better fashion....\n", "    # Obspacks: https://data.icos-cp.eu/objects/UqPhG00TNqHmcRybZ1e43ZX9 (1972-2023)  SQxOn3waZ55FjDKxcXI41xVD (1972-2022)\n", "    # if(\"SQxOn3waZ55FjDKxcXI41xVD\" in finalDobjLst):\n", "    #    finalDobjLst.remove(\"SQxOn3waZ55FjDKxcXI41xVD\")\n", "    # pid=\"UqPhG00TNqHmcRybZ1e43ZX9\"\n", "    collpid=\"5-kp-zFm31bQs47leuuGCBTZ\"\n", "    # if(\"European Obspack compilation of atmospheric carbon dioxide data\" in meta['specificInfo']['title'])\n", "    while(1<0) :  # TODO wait fur bugfix in ocscp Lib     (len(pid)>10):\n", "        pidUrl=\"https://meta.icos-cp.eu/objects/\"+collpid\n", "        if(collpid in finalDobjLst):\n", "            finalDobjLst.remove(collpid)\n", "            logger.info(f\"Rejecting pidUrl: {pidUrl}    European_Obspack_compilation_of_atmospheric_carbon_dioxide_data in favour of its more up-to-date individual data records.\")\n", "        dob = Dobj(pidUrl)  # TODO: crashes with pidUrl=https://meta.icos-cp.eu/objects/UqPhG00TNqHmcRybZ1e43ZX9     -- Why??\n", "        logger.debug(f\"dobj: {dob}\")\n", "        dobnext=dob.next\n", "        if(dobnext):\n", "            pidUrl=dob.next\n", "            pid=pidUrl.split('/')[-1]\n", "        else:\n", "            pid=\"\"\n", "        \n", "    # 5-kp-zFm31bQs47leuuGCBTZ  same as https://doi.org/10.18160/PEKQ-M4T1 which is a newer version of the above package\n", "    # if(\"5-kp-zFm31bQs47leuuGCBTZ\" in finalDobjLst):\n", "    #     finalDobjLst.remove(\"5-kp-zFm31bQs47leuuGCBTZ\")\n", "    # if(\"UqPhG00TNqHmcRybZ1e43ZX9\" in finalDobjLst):\n", "    #     finalDobjLst.remove(\"UqPhG00TNqHmcRybZ1e43ZX9\")\n", "    lf=len(finalDobjLst)\n", "    logger.info(f\"Found {lf} valid data objects on the carbon portal (dry mole fraction observation files for chosen tracer).\")\n", "    \n", "    # Create a dataframe with meaningful parameters from which a user may filter subsets\n", "    with open( r'ListOfAllValidPIDs.txt', 'w') as fp:\n", "        for item in finalDobjLst:\n", "            fp.write(\"%s\\n\" % item)\n", "    i=0\n", "    df=DataFrame()\n", "    bSelected=True\n", "    nObj=len(finalDobjLst)\n", "    step=int(0.5+(nObj/50.0))\n", "    #print('|_________________________________________________|')\n", "    printProgressBar(0, nObj, prefix = 'Gathering meta data progress:', suffix = 'Done', length = 50)\n", "    for n, pid in enumerate(finalDobjLst):\n", "        pidMetadata = metadata.get(\"https://meta.icos-cp.eu/objects/\"+pid)\n", "        if pidMetadata is not None:\n", "            isICOS=False\n", "            if(pidMetadata['specification'].get('keywords', None) is not None):\n", "                isICOS = any('ICOS' in sKwrd for sKwrd in pidMetadata['specification'].get('keywords', None))\n", "            if(not isICOS):\n", "                if(pidMetadata['references'].get('keywords', None) is not None):\n", "                    isICOS = any('ICOS' in sKwrd for sKwrd in pidMetadata['references'].get('keywords', None))\n", "            if(isICOS==False):\n", "                logger.debug(f\"This pidMetadata does not say that it is ICOS: {pid}\")\n", "            data=[pid, bSelected,  pidMetadata['specificInfo']['acquisition']['station']['id'], \n", "                        pidMetadata['specificInfo']['acquisition']['station']['countryCode'],\n", "                        isICOS, \n", "                        pidMetadata['coverageGeo']['geometry']['coordinates'][1], \n", "                        pidMetadata['coverageGeo']['geometry']['coordinates'][0], \n", "                        pidMetadata['coverageGeo']['geometry']['coordinates'][2],\n", "                        pidMetadata['specificInfo']['acquisition']['samplingHeight'], \n", "                        pidMetadata['size'], \n", "                        pidMetadata['specificInfo']['nRows'], \n", "                        pidMetadata['specification']['dataLevel'], \n", "                        pidMetadata['specificInfo']['acquisition']['interval']['start'],\n", "                        pidMetadata['specificInfo']['acquisition']['interval']['stop'],\n", "                        pidMetadata['specificInfo']['productionInfo']['dateTime'], \n", "                        pidMetadata['accessUrl'],\n", "                        pidMetadata['fileName'], int(0), \n", "                        pidMetadata['specification']['self']['label']]\n", "            if(i==0):\n", "                '''\n", "                stationID=pidMetadata['specificInfo']['acquisition']['station']['id']\n", "                country=pidMetadata['specificInfo']['acquisition']['station']['countryCode']\n", "                isICOS: keyWrds (any 'ICOS' 'CO2'/'CH4'   in pidMetadata['specification']['keywords'] / pidMetadata['references']['keywords'] (List)  \n", "                # Tracer: keyWrds (any  'CO2'/'CH4'   in pidMetadata['specification']['keywords'] / pidMetadata['references']['keywords'] (List)  \n", "                lon=pidMetadata['coverageGeo']['geometry']['coordinates'][0]\n", "                lat=pidMetadata['coverageGeo']['geometry']['coordinates'][1]\n", "                alt=pidMetadata['coverageGeo']['geometry']['coordinates'][2]\n", "                samplingHeight = pidMetadata['specificInfo']['acquisition']['samplingHeight']\n", "                size=pidMetadata['size']   \n", "                nRows=pidMetadata['specificInfo']['nRows']\n", "                dataLevel=pidMetadata['specification']['dataLevel']\n", "                obsStart=pidMetadata['specificInfo']['acquisition']['interval']['start']\n", "                obsStop=pidMetadata['specificInfo']['acquisition']['interval']['stop']\n", "                  #tmporalCoverage=pidMetadata['references']['temporalCoverageDisplay']\n", "                productionTime=pidMetadata['specificInfo']['productionInfo']['dateTime']\n", "                sUrl=pidMetadata['accessUrl']\n", "                fileName=pidMetadata['fileName']\n", "                dataSetLabel=pidMetadata['specification']['self']['label']  \n", "                '''\n", "                columnNames=['pid', 'selected','stationID', 'country', 'isICOS','latitude','longitude','altitude','samplingHeight','size', \n", "                        'nRows','dataLevel','obsStart','obsStop','productionTime','accessUrl','fileName','dClass','dataSetLabel'] \n", "                #data=[[pid, pidMetadata['specificInfo']['acquisition']['station']['id'], pidMetadata['coverageGeo']['geometry']['coordinates'][0], pidMetadata['coverageGeo']['geometry']['coordinates'][1], pidMetadata['coverageGeo']['geometry']['coordinates'][2], pidMetadata['specificInfo']['acquisition']['samplingHeight'], pidMetadata['size'], pidMetadata['specification']['dataLevel'], pidMetadata['references']['temporalCoverageDisplay'], pidMetadata['specificInfo']['productionInfo']['dateTime'], pidMetadata['accessUrl'], pidMetadata['fileName'], int(0), pidMetadata['specification']['self']['label']]]\n", "                df=DataFrame(data=[data], columns=columnNames)\n", "            else:\n", "                #data=[pid, pidMetadata['specificInfo']['acquisition']['station']['id'], pidMetadata['coverageGeo']['geometry']['coordinates'][0], pidMetadata['coverageGeo']['geometry']['coordinates'][1], pidMetadata['coverageGeo']['geometry']['coordinates'][2], pidMetadata['specificInfo']['acquisition']['samplingHeight'], pidMetadata['size'], pidMetadata['specification']['dataLevel'], pidMetadata['references']['temporalCoverageDisplay'], pidMetadata['specificInfo']['productionInfo']['dateTime'], pidMetadata['accessUrl'], pidMetadata['fileName'], int(0), pidMetadata['specification']['self']['label']]\n", "                if(len(df.columns)==len(data)):\n", "                    df.loc[len(df)] = data\n", "                else:\n", "                    logger.error(f\"corrupted data set: carbon portal data record with PID={pid}. Missing meta data. This data set will not be used in this run.\")\n", "                \n", "            i+=1\n", "        if(n % step ==0):\n", "            printProgressBar(n, nObj, prefix = 'Gathering meta data progress:', suffix = 'Done', length = 50)\n", "    \n", "    df['dClass'] = int(0) # initialise unknown data quality\n", "    df['dClass'] = np.where(df.dataSetLabel.str.contains(\"Obspack\", flags=re.IGNORECASE), int(4), int(0))\n", "    df['dClass'] = np.where(df.dataSetLabel.str.contains(\"Release\", flags=re.IGNORECASE), int(3), df['dClass'] )\n", "    df['dClass'] = np.where(df.dataSetLabel.str.contains(\"product\", flags=re.IGNORECASE), int(2), df['dClass'] )\n", "    df['dClass'] = np.where(df.dataSetLabel.str.contains(\"NRT \"), int(1), df['dClass'] )\n", "    df.to_csv('dfValidObsUnsorted.csv', mode='w', sep=',')\n", "    dfCountStations=df.drop_duplicates(['stationID'], keep='first') \n", "    nObsDataRecords = len(df)\n", "    nTotalStations=len(dfCountStations)\n", "    \n", "    # Only include observations made within the selected geographical region\n", "    Lat0=ymlContents['run']['region']['lat0']\n", "    Lat1=ymlContents['run']['region']['lat1'] \n", "    Lon0=ymlContents['run']['region']['lon0']\n", "    Lon1=ymlContents['run']['region']['lon1']\n", "    filtered = ((df['latitude'] >= Lat0) &\n", "           (df['latitude'] <= Lat1) & \n", "           (df['longitude'] >= Lon0) & \n", "           (df['longitude'] <= Lon1))\n", "    dfq= df[filtered]\n", "    dfCountStations2=dfq.drop_duplicates(['stationID'], keep='first') \n", "    nObsDataRecords2 = len(dfq)\n", "    nTotalStations2=len(dfCountStations2)\n", "    nRemovedStations=nTotalStations - nTotalStations2\n", "    ndiff=nObsDataRecords - nObsDataRecords2\n", "    logger.info(f\"{nTotalStations} observation sites found. Thereof {nRemovedStations} fall outside the geographical region selected.\")\n", "    logger.info(f\"{nObsDataRecords} observational data records found. Thereof {ndiff} fall outside the geographical region selected.\")\n", "    logger.info(f\"{nTotalStations2} observation sites remaining. {nObsDataRecords2} valid observational data records remaining.\")\n", "    dfq.sort_values(by = ['country','stationID', 'dClass', 'samplingHeight', 'productionTime'], inplace = True, ascending = [True, True, False, False, False])\n", "    dfq.to_csv('dfValidObs.csv', mode='w', sep=',')\n", "    dfqdd=dfq.drop_duplicates(['stationID', 'dClass', 'samplingHeight'], keep='first')  # discards older  'productionTime' datasets\n", "    logger.info(\"Dropping duplicates and deprecated data sets that have been replaced with newer versions.\")\n", "    # But we are still keeping all sampling heights.\n", "    sLogCfgPath=\"\"\n", "    if ((ymlContents['run']['paths']['output'] is None) or len(ymlContents['run']['paths']['output']))<1:\n", "        sLogCfgPath=\"./\"\n", "    else:\n", "        sLogCfgPath=ymlContents['run']['paths']['output']+\"/\"\n", "    fDiscoveredObservations=sLogCfgPath+\"Lumia-\"+sNow+\"-DiscoveredObservations.csv\"\n", "    nObsDataRecords2 = len(dfqdd)\n", "    logger.info(f\"{nObsDataRecords2} valid observational data records remaining from {nTotalStations2} stations across Europe.\")\n", "    dfqdd.to_csv(fDiscoveredObservations, mode='w', sep=',')\n", "    selectedDobjCol=dfqdd['pid']\n", "    selectedDobjLst = selectedDobjCol.iloc[1:].tolist()\n", "    return(finalDobjLst, selectedDobjLst, dfqdd, fDiscoveredObservations)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*****************************************************************************************************************************"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def chooseAmongDiscoveredObservations(bWithGui=True, tracer='CO2', ValidObs=None, ymlFile=None, fDiscoveredObservations=None, \n", "                                                                            sNow='', selectedObsFile='',   bSkipGui=False,  iVerbosityLv=1):\n", "# *****************************************************************************************************************************\n\n", "    # Shall we call the GUI to tweak some parameters before we start the ball rolling?\n", "    if bWithGui:\n", "        #(updatedYmlContents) = callLumiaGUI(ymlContents, sLogCfgPath)\n", "        # callLumiaGUI(rcf, args.start,  args.end )\n", "        script_directory = os.path.dirname(os.path.abspath(sys.argv[0]))\n", "        sCmd ='python3 '+script_directory+'/lumia/GUI/lumiaGUI.py --step2 --DiscoveredObs='+fDiscoveredObservations\n", "        sCmd+=' --sNow='+sNow\n", "        for entry in sys.argv[1:]:\n", "            if (len(entry)>0):\n", "                sCmd+=' '+entry\n", "        try:\n", "            returnValue=os.system(sCmd)\n", "        except:\n", "            logger.error(f\"Calling LumiaGUI failed. {returnValue} Execution stopped.\")\n", "            sys.exit(42)\n", "        logger.info(\"LumiaGUI window closed\")\n", "        if(os.path.isfile(\"LumiaGui.stop\")):\n", "            logger.error(\"The user canceled the call of Lumia or something went wrong in the Refinement GUI. Execution aborted. Lumia was not called.\")\n", "            sys.exit(42)\n", "    ValidObs.read_csv(selectedObsFile)\n", "    # Read the ymlFile\n", "    # Apply all filters found in the ymlFile\n\n", "    # Write the resulting list of chosen obsDataSetgs to \"ObservationsSelected4Lumia.csv\"\n", "    # chosenObs=ValidObs.where(ValidObs['selected']==True)\n", "    chosen = (ValidObs['selected']==True)\n", "    chosenObs= ValidObs[chosen]\n", "    \n", "    selectedDobjCol=chosenObs['pid']\n", "    selectedDobjLst = selectedDobjCol.iloc[1:].tolist()\n", "    return(selectedDobjLst, chosenObs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print iterations progress"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '\u2588', printEnd = \"\\r\"):\n", "    \"\"\"\n", "    Call in a loop to create terminal progress bar\n", "    @params:\n", "        iteration   - Required  : current iteration (Int)\n", "        total       - Required  : total iterations (Int)\n", "        prefix      - Optional  : prefix string (Str)\n", "        suffix      - Optional  : suffix string (Str)\n", "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n", "        length      - Optional  : character length of bar (Int)\n", "        fill        - Optional  : bar fill character (Str)\n", "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n", "    \"\"\"\n", "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n", "    filledLength = int(length * iteration // total)\n", "    bar = fill * filledLength + '-' * (length - filledLength)\n", "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n", "    # Print New Line on Complete\n", "    if iteration == total: \n", "        print()\n", "    "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}